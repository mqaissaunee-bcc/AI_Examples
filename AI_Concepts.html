<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Important AI Concepts to Know - Interactive Learning</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }
        
        .header {
            background: linear-gradient(135deg, #8B5CF6 0%, #3B82F6 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }
        
        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            font-weight: 700;
        }
        
        .header p {
            font-size: 1.2rem;
            opacity: 0.9;
            margin-bottom: 15px;
        }
        
        .learning-path {
            background: rgba(255,255,255,0.2);
            border-radius: 15px;
            padding: 20px;
            margin-top: 20px;
        }
        
        .learning-path h3 {
            font-size: 1.3rem;
            margin-bottom: 15px;
        }
        
        .path-stages {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }
        
        .path-stage {
            background: rgba(255,255,255,0.1);
            border-radius: 10px;
            padding: 15px;
            text-align: center;
        }
        
        .path-stage h4 {
            font-size: 1rem;
            margin-bottom: 8px;
            color: #FFF3A0;
        }
        
        .path-stage p {
            font-size: 0.9rem;
            opacity: 0.9;
        }
        
        .grid-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            padding: 30px;
        }
        
        .concept-card {
            background: white;
            border: 2px solid #e5e7eb;
            border-radius: 15px;
            padding: 20px;
            cursor: pointer;
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }
        
        .concept-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(0,0,0,0.15);
            border-color: #8B5CF6;
        }
        
        .concept-card.active {
            border-color: #8B5CF6;
            box-shadow: 0 0 0 3px rgba(139, 92, 246, 0.2);
        }
        
        .concept-number {
            position: absolute;
            top: -10px;
            left: -10px;
            background: #1f2937;
            color: white;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 1.1rem;
        }
        
        .concept-title {
            background: #fbbf24;
            color: #1f2937;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 1rem;
            margin: 10px 0 15px 0;
            display: inline-block;
        }
        
        .concept-icon {
            font-size: 3rem;
            margin: 15px 0;
            text-align: center;
            height: 80px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .concept-description {
            color: #6b7280;
            font-size: 0.95rem;
            line-height: 1.4;
        }
        
        .stage-indicator {
            position: absolute;
            top: 10px;
            right: 10px;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 0.75rem;
            font-weight: 600;
            text-transform: uppercase;
        }
        
        .stage-foundation { background: #dbeafe; color: #1e40af; }
        .stage-core { background: #dcfce7; color: #166534; }
        .stage-modern { background: #fef3c7; color: #92400e; }
        .stage-techniques { background: #f3e8ff; color: #6b21a8; }
        .stage-infrastructure { background: #f0f9ff; color: #0c4a6e; }
        .stage-advanced { background: #fecaca; color: #991b1b; }
        
        .modal {
            display: none;
            position: fixed;
            z-index: 1000;
            left: 0;
            top: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0,0,0,0.8);
            backdrop-filter: blur(5px);
        }
        
        .modal-content {
            background-color: white;
            margin: 2% auto;
            padding: 0;
            border-radius: 20px;
            width: 90%;
            max-width: 800px;
            max-height: 90vh;
            overflow-y: auto;
            position: relative;
            animation: modalSlideIn 0.3s ease;
        }
        
        @keyframes modalSlideIn {
            from { transform: translateY(-50px); opacity: 0; }
            to { transform: translateY(0); opacity: 1; }
        }
        
        .modal-header {
            background: linear-gradient(135deg, #8B5CF6 0%, #3B82F6 100%);
            color: white;
            padding: 30px;
            border-radius: 20px 20px 0 0;
        }
        
        .modal-title {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 10px;
        }
        
        .modal-subtitle {
            font-size: 1.2rem;
            opacity: 0.9;
        }
        
        .modal-body {
            padding: 30px;
        }
        
        .close {
            position: absolute;
            right: 20px;
            top: 20px;
            color: white;
            font-size: 2rem;
            font-weight: bold;
            cursor: pointer;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background: rgba(255,255,255,0.2);
            display: flex;
            align-items: center;
            justify-content: center;
            transition: background 0.3s ease;
        }
        
        .close:hover {
            background: rgba(255,255,255,0.3);
        }
        
        .lesson-section {
            margin-bottom: 25px;
        }
        
        .lesson-section h3 {
            color: #1f2937;
            font-size: 1.3rem;
            margin-bottom: 15px;
            padding-bottom: 8px;
            border-bottom: 2px solid #e5e7eb;
        }
        
        .lesson-section p {
            color: #4b5563;
            line-height: 1.6;
            margin-bottom: 12px;
        }
        
        .example-box {
            background: #f3f4f6;
            border-left: 4px solid #8B5CF6;
            padding: 20px;
            margin: 15px 0;
            border-radius: 0 10px 10px 0;
        }
        
        .example-box h4 {
            color: #8B5CF6;
            margin-bottom: 10px;
            font-size: 1.1rem;
        }
        
        .key-points {
            background: #eff6ff;
            border: 1px solid #3b82f6;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
        }
        
        .key-points h4 {
            color: #1e40af;
            margin-bottom: 10px;
        }
        
        .key-points ul {
            margin-left: 20px;
        }
        
        .key-points li {
            margin-bottom: 5px;
            color: #1e40af;
        }
        
        .keyboard-controls {
            position: absolute;
            bottom: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.9);
            color: white;
            padding: 18px 22px;
            border-radius: 12px;
            font-size: 0.9rem;
            z-index: 1002;
            transition: all 0.3s ease;
            display: none;
            border: 1px solid rgba(255, 255, 255, 0.2);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        }
        
        .keyboard-controls.show {
            display: block !important;
            animation: slideInControls 0.4s ease-out;
            opacity: 1;
        }
        
        @keyframes slideInControls {
            from {
                opacity: 0;
                transform: translateY(20px) scale(0.95);
            }
            to {
                opacity: 1;
                transform: translateY(0) scale(1);
            }
        }
        
        .keyboard-controls h4 {
            margin: 0 0 12px 0;
            font-size: 1rem;
            color: #fbbf24;
            font-weight: 600;
        }
        
        .control-item {
            display: flex;
            align-items: center;
            margin-bottom: 10px;
        }
        
        .control-item:last-child {
            margin-bottom: 0;
        }
        
        .key {
            background: linear-gradient(145deg, rgba(255, 255, 255, 0.25), rgba(255, 255, 255, 0.15));
            border: 1px solid rgba(255, 255, 255, 0.4);
            border-radius: 6px;
            padding: 4px 10px;
            margin-right: 12px;
            font-family: 'SF Mono', Monaco, 'Cascadia Code', monospace;
            font-size: 0.85rem;
            min-width: 28px;
            text-align: center;
            font-weight: 600;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
        }
        
        .navigation-hint {
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            color: white;
            font-size: 0.9rem;
            opacity: 0.8;
            display: flex;
            align-items: center;
            gap: 15px;
        }
        
        .nav-arrow {
            background: rgba(255, 255, 255, 0.2);
            border-radius: 6px;
            padding: 5px 10px;
            cursor: pointer;
            transition: background 0.3s ease;
        }
        
        .nav-arrow:hover {
            background: rgba(255, 255, 255, 0.3);
        }
        
        .nav-arrow.disabled {
            opacity: 0.4;
            cursor: not-allowed;
        }
        
        .nav-arrow.disabled:hover {
            background: rgba(255, 255, 255, 0.2);
        }
        
        @media (max-width: 768px) {
            .grid-container {
                grid-template-columns: 1fr;
                padding: 20px;
            }
            
            .header h1 {
                font-size: 2rem;
            }
            
            .modal-content {
                width: 95%;
                margin: 5% auto;
            }
            
            .path-stages {
                grid-template-columns: 1fr;
            }
            
            .keyboard-controls {
                bottom: 10px;
                right: 10px;
                padding: 12px 16px;
                font-size: 0.8rem;
            }
            
            .navigation-hint {
                bottom: 15px;
                font-size: 0.8rem;
                gap: 10px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üß† Important AI Concepts to Know</h1>
            <p>Interactive Learning Experience - Click any concept to dive deeper</p>
            
            <div class="learning-path">
                <h3>üìö Structured Learning Path</h3>
                <div class="path-stages">
                    <div class="path-stage">
                        <h4>Foundation (1-6)</h4>
                        <p>Core ML concepts and learning types</p>
                    </div>
                    <div class="path-stage">
                        <h4>Core AI (7-12)</h4>
                        <p>Essential AI technologies and techniques</p>
                    </div>
                    <div class="path-stage">
                        <h4>Modern Models (13-18)</h4>
                        <p>Current AI models and architectures</p>
                    </div>
                    <div class="path-stage">
                        <h4>Techniques (19-23)</h4>
                        <p>Optimization and enhancement methods</p>
                    </div>
                    <div class="path-stage">
                        <h4>Infrastructure (24-26)</h4>
                        <p>Production and operational systems</p>
                    </div>
                    <div class="path-stage">
                        <h4>Future (27-29)</h4>
                        <p>Advanced concepts and future directions</p>
                    </div>
                </div>
            </div>
        </div>
        
        <div class="grid-container" id="conceptGrid">
            <!-- Concepts will be dynamically generated -->
        </div>
    </div>
    
    <!-- Keyboard Controls Overlay -->
    <div class="keyboard-controls" id="keyboardControls">
        <h4>‚å®Ô∏è Keyboard Controls</h4>
        <div class="control-item">
            <span class="key">Esc</span>
            <span>Close concept</span>
        </div>
        <div class="control-item">
            <span class="key">‚Üê</span>
            <span>Previous concept</span>
        </div>
        <div class="control-item">
            <span class="key">‚Üí</span>
            <span>Next concept</span>
        </div>
    </div>
    
    <!-- Modal -->
    <div id="conceptModal" class="modal">
        <div class="modal-content">
            <div class="modal-header">
                <span class="close">&times;</span>
                <div class="modal-title" id="modalTitle"></div>
                <div class="modal-subtitle" id="modalSubtitle"></div>
                <div class="navigation-hint">
                    <div class="nav-arrow" id="prevConcept">‚Üê Previous</div>
                    <span id="conceptPosition"></span>
                    <div class="nav-arrow" id="nextConcept">Next ‚Üí</div>
                </div>
            </div>
            <div class="modal-body" id="modalBody"></div>
            
            <!-- Keyboard Controls Overlay - inside modal -->
            <div class="keyboard-controls" id="keyboardControlsModal">
                <h4>‚å®Ô∏è Keyboard Controls</h4>
                <div class="control-item">
                    <span class="key">Esc</span>
                    <span>Close concept</span>
                </div>
                <div class="control-item">
                    <span class="key">‚Üê</span>
                    <span>Previous concept</span>
                </div>
                <div class="control-item">
                    <span class="key">‚Üí</span>
                    <span>Next concept</span>
                </div>
            </div>
        </div>
    </div>

    <script>
        const concepts = [
            // FOUNDATIONAL CONCEPTS (1-6)
            {
                number: 1,
                title: "Machine Learning",
                icon: "ü§ñ",
                stage: "foundation",
                description: "Core algorithms, statistics, and model training techniques",
                lesson: {
                    subtitle: "The Foundation of Modern AI",
                    sections: [
                        {
                            title: "What is Machine Learning?",
                            content: "Machine Learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed for every task. Instead of following pre-written instructions, ML systems identify patterns in data and use these patterns to make predictions or decisions on new, unseen data."
                        },
                        {
                            title: "Core Components",
                            content: "Machine Learning involves three key elements: algorithms (the mathematical methods), data (the information used for training), and models (the trained systems that make predictions). The process typically involves training a model on historical data, validating its performance, and then deploying it to make predictions on new data."
                        },
                        {
                            title: "Types of Machine Learning",
                            content: "There are three main types: Supervised Learning (learning with labeled examples), Unsupervised Learning (finding patterns in unlabeled data), and Reinforcement Learning (learning through trial and error with rewards and penalties)."
                        }
                    ],
                    example: {
                        title: "Real-world Example",
                        content: "Email spam detection is a classic ML application. The system learns from thousands of emails labeled as 'spam' or 'not spam', identifying patterns like certain words, sender addresses, or formatting. Once trained, it can automatically classify new emails with high accuracy."
                    },
                    keyPoints: [
                        "Learns patterns from data automatically",
                        "Improves performance with more data",
                        "Foundation for most modern AI applications",
                        "Requires quality data for good results"
                    ]
                }
            },
            {
                number: 2,
                title: "Neural Networks",
                icon: "üï∏Ô∏è",
                stage: "foundation",
                description: "Layered architectures efficiently model non-linear relationships accurately",
                lesson: {
                    subtitle: "The Building Blocks of AI",
                    sections: [
                        {
                            title: "Neural Network Basics",
                            content: "Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) that process and transmit information. Each connection has a weight that determines the strength of the signal passed between neurons."
                        },
                        {
                            title: "How They Learn",
                            content: "Neural networks learn through a process called backpropagation. During training, the network makes predictions, compares them to the correct answers, and adjusts the connection weights to reduce errors. This process repeats thousands of times until the network becomes accurate."
                        },
                        {
                            title: "Architecture Types",
                            content: "Different neural network architectures serve different purposes: Feedforward networks for basic classification, Convolutional Neural Networks (CNNs) for image processing, Recurrent Neural Networks (RNNs) for sequential data, and Transformer networks for language processing."
                        }
                    ],
                    example: {
                        title: "Voice Recognition",
                        content: "When you speak to Siri or Alexa, neural networks convert your voice into text. The network processes the audio waveform, identifies phonemes (basic speech sounds), combines them into words, and interprets the meaning - all in real-time."
                    },
                    keyPoints: [
                        "Inspired by biological brain structure",
                        "Learn through adjusting connection weights",
                        "Can model complex non-linear relationships",
                        "Foundation for most modern AI systems"
                    ]
                }
            },
            {
                number: 3,
                title: "Deep Learning",
                icon: "üß†",
                stage: "foundation",
                description: "Hierarchical neural networks learning complex representations automatically",
                lesson: {
                    subtitle: "Multi-layered Neural Networks",
                    sections: [
                        {
                            title: "Understanding Deep Learning",
                            content: "Deep Learning is a specialized subset of machine learning that uses artificial neural networks with multiple layers (hence 'deep') to model and understand complex patterns in data. These networks are inspired by how the human brain processes information through interconnected neurons."
                        },
                        {
                            title: "The Power of Layers",
                            content: "Each layer in a deep neural network learns increasingly complex features. The first layers might detect simple edges in images, middle layers identify shapes and textures, while deeper layers recognize complete objects like faces or cars. This hierarchical learning is what makes deep learning so powerful."
                        },
                        {
                            title: "Why Deep Learning Works",
                            content: "Deep learning excels at automatic feature extraction. Traditional machine learning often requires manual feature engineering, but deep learning automatically discovers the most relevant features from raw data. This makes it particularly effective for complex data like images, speech, and text."
                        }
                    ],
                    example: {
                        title: "Image Recognition Example",
                        content: "In medical imaging, deep learning models can analyze X-rays or MRI scans to detect diseases. The network learns to identify subtle patterns that might indicate conditions like pneumonia or tumors, often achieving accuracy levels comparable to or exceeding human specialists."
                    },
                    keyPoints: [
                        "Uses multiple layers of artificial neurons",
                        "Automatically learns complex features",
                        "Excels at pattern recognition in complex data",
                        "Requires large amounts of data and computational power"
                    ]
                }
            },
            {
                number: 4,
                title: "Supervised Learning",
                icon: "üìö",
                stage: "foundation",
                description: "Learns useful representations with labeled data",
                lesson: {
                    subtitle: "Learning with Labeled Examples",
                    sections: [
                        {
                            title: "Supervised Learning Fundamentals",
                            content: "Supervised learning uses labeled training data to learn a mapping from inputs to outputs. The algorithm learns from examples where both the input and the correct answer (label) are provided, allowing it to make predictions on new, unseen data."
                        },
                        {
                            title: "Types of Supervised Learning",
                            content: "Classification tasks predict discrete categories (email is spam or not spam), while regression tasks predict continuous values (house prices, stock prices). Both use labeled training data but have different output types and evaluation metrics."
                        },
                        {
                            title: "Common Algorithms",
                            content: "Popular supervised learning algorithms include linear regression, decision trees, random forests, support vector machines, and neural networks. Each has strengths and weaknesses depending on the data type and problem complexity."
                        }
                    ],
                    example: {
                        title: "Medical Diagnosis",
                        content: "Supervised learning can help diagnose diseases by training on medical records with known diagnoses. The system learns patterns between symptoms, test results, and diagnoses, then can suggest diagnoses for new patients based on their symptoms."
                    },
                    keyPoints: [
                        "Uses labeled training data",
                        "Includes classification and regression tasks",
                        "Most common machine learning approach",
                        "Requires quality labeled examples for training"
                    ]
                }
            },
            {
                number: 5,
                title: "Reinforcement Learning",
                icon: "üéØ",
                stage: "foundation",
                description: "Agent learns optimal behavior through interaction with environment",
                lesson: {
                    subtitle: "Learning Through Trial and Error",
                    sections: [
                        {
                            title: "Reinforcement Learning Principles",
                            content: "Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions and learns to maximize cumulative rewards over time through trial and error."
                        },
                        {
                            title: "Key Components",
                            content: "RL systems consist of an agent (the learner), an environment (the world the agent interacts with), actions (choices the agent can make), states (situations the agent can be in), and rewards (feedback signals). The agent's goal is to learn an optimal policy - a strategy for choosing actions."
                        },
                        {
                            title: "Learning Process",
                            content: "The agent starts with random actions and gradually improves through experience. It uses techniques like Q-learning or policy gradients to update its decision-making strategy based on the rewards received. Over time, it learns which actions lead to better outcomes in different situations."
                        }
                    ],
                    example: {
                        title: "Game Playing AI",
                        content: "AlphaGo, the AI that defeated world champions in Go, used reinforcement learning. It played millions of games against itself, learning from wins and losses. Each game provided feedback that helped it develop increasingly sophisticated strategies."
                    },
                    keyPoints: [
                        "Learns optimal behavior through interaction",
                        "Uses rewards and penalties as feedback",
                        "Excellent for sequential decision-making",
                        "Applications in robotics, game AI, and optimization"
                    ]
                }
            },
            {
                number: 6,
                title: "Bayesian Learning",
                icon: "üìä",
                stage: "foundation",
                description: "Incorporates uncertainty using probabilistic model approaches",
                lesson: {
                    subtitle: "Probabilistic Approach to Learning",
                    sections: [
                        {
                            title: "Bayesian Learning Principles",
                            content: "Bayesian learning incorporates uncertainty and prior knowledge into machine learning. It uses Bayes' theorem to update beliefs about model parameters as new data arrives, providing not just predictions but also confidence estimates."
                        },
                        {
                            title: "Prior Knowledge Integration",
                            content: "Unlike other approaches, Bayesian methods can incorporate existing knowledge through prior distributions. This is especially valuable when data is limited or when domain expertise suggests certain parameter values are more likely."
                        },
                        {
                            title: "Uncertainty Quantification",
                            content: "Bayesian models provide uncertainty estimates with their predictions. Instead of just saying 'the price will be $100K', they might say 'the price will be $100K with 95% confidence interval of $90K-$110K', which is valuable for decision-making."
                        }
                    ],
                    example: {
                        title: "Drug Discovery",
                        content: "In pharmaceutical research, Bayesian methods help optimize drug trials by incorporating prior knowledge about similar compounds and continuously updating beliefs about drug effectiveness as trial data becomes available."
                    },
                    keyPoints: [
                        "Incorporates prior knowledge and uncertainty",
                        "Provides confidence estimates with predictions",
                        "Updates beliefs as new data arrives",
                        "Valuable when data is limited or noisy"
                    ]
                }
            },
            
            // CORE AI TECHNOLOGIES (7-12)
            {
                number: 7,
                title: "Computer Vision",
                icon: "üëÅÔ∏è",
                stage: "core",
                description: "Algorithms interpreting and analyzing visual data effectively",
                lesson: {
                    subtitle: "Teaching Machines to See",
                    sections: [
                        {
                            title: "Computer Vision Fundamentals",
                            content: "Computer Vision enables machines to interpret and understand visual information from the world. It processes digital images and videos to extract meaningful information, identify objects, understand scenes, and make decisions based on visual input."
                        },
                        {
                            title: "Core Techniques",
                            content: "Computer vision uses various techniques including image preprocessing (noise reduction, enhancement), feature detection (edges, corners, textures), object detection and recognition, image segmentation (dividing images into regions), and 3D reconstruction."
                        },
                        {
                            title: "Modern Approaches",
                            content: "Contemporary computer vision heavily relies on deep learning, particularly Convolutional Neural Networks (CNNs). These networks can automatically learn visual features and achieve human-level performance in many vision tasks."
                        }
                    ],
                    example: {
                        title: "Autonomous Vehicles",
                        content: "Self-driving cars use computer vision to navigate roads safely. They identify lane markings, detect other vehicles, recognize traffic signs and signals, spot pedestrians, and assess road conditions - all in real-time to make driving decisions."
                    },
                    keyPoints: [
                        "Enables machines to interpret visual information",
                        "Uses deep learning for feature extraction",
                        "Applications in robotics, healthcare, security",
                        "Combines multiple visual processing techniques"
                    ]
                }
            },
            {
                number: 8,
                title: "NLP",
                icon: "üí¨",
                stage: "core",
                description: "Techniques to process and understand natural language complex text",
                lesson: {
                    subtitle: "Teaching Machines to Understand Language",
                    sections: [
                        {
                            title: "What is Natural Language Processing?",
                            content: "Natural Language Processing (NLP) is the field of AI that focuses on enabling computers to understand, interpret, and generate human language. It bridges the gap between human communication and computer understanding, allowing machines to process text and speech in meaningful ways."
                        },
                        {
                            title: "Core NLP Tasks",
                            content: "NLP encompasses various tasks including text classification (categorizing documents), named entity recognition (identifying people, places, organizations), sentiment analysis (determining emotional tone), machine translation (converting between languages), and text summarization."
                        },
                        {
                            title: "Challenges in NLP",
                            content: "Human language is inherently complex with ambiguity, context-dependence, idioms, and cultural nuances. NLP systems must handle these challenges while understanding grammar, semantics, and pragmatics - the study of language in context."
                        }
                    ],
                    example: {
                        title: "Customer Service Chatbots",
                        content: "Modern chatbots use NLP to understand customer queries, extract intent and entities, and provide relevant responses. They can handle multiple languages, understand context across conversation turns, and escalate complex issues to human agents when needed."
                    },
                    keyPoints: [
                        "Enables computers to process human language",
                        "Handles text analysis, generation, and translation",
                        "Requires understanding of context and meaning",
                        "Powers virtual assistants and translation services"
                    ]
                }
            },
            {
                number: 9,
                title: "Transformers",
                icon: "‚ö°",
                stage: "core",
                description: "Self-attention-based architecture powering modern AI models",
                lesson: {
                    subtitle: "The Architecture That Changed AI",
                    sections: [
                        {
                            title: "Transformer Architecture",
                            content: "Transformers are a neural network architecture that revolutionized AI by using self-attention mechanisms instead of recurrent connections. They can process all positions in a sequence simultaneously, making them more efficient and effective for language tasks."
                        },
                        {
                            title: "Self-Attention Mechanism",
                            content: "Self-attention allows the model to relate different positions in a sequence to compute representations. When processing a word, the model can attend to all other words in the sentence, understanding context and relationships regardless of distance."
                        },
                        {
                            title: "Impact on AI",
                            content: "Transformers enabled the development of large language models like GPT and BERT. Their parallel processing capabilities and ability to capture long-range dependencies made them the foundation for most modern NLP breakthroughs."
                        }
                    ],
                    example: {
                        title: "Machine Translation",
                        content: "Google Translate uses transformer models that can translate between languages by understanding the meaning and context of entire sentences, rather than translating word-by-word. This produces more natural and accurate translations."
                    },
                    keyPoints: [
                        "Uses self-attention instead of recurrence",
                        "Processes sequences in parallel",
                        "Foundation for GPT, BERT, and similar models",
                        "Revolutionized natural language processing"
                    ]
                }
            },
            {
                number: 10,
                title: "Embeddings",
                icon: "üîó",
                stage: "core",
                description: "Transforms input into machine-readable vector formats",
                lesson: {
                    subtitle: "Converting Data to Mathematical Representations",
                    sections: [
                        {
                            title: "Understanding Embeddings",
                            content: "Embeddings are dense vector representations of data that capture semantic meaning in a mathematical form. They convert discrete items like words, images, or users into continuous vectors where similar items are positioned close together in the vector space."
                        },
                        {
                            title: "How Embeddings Work",
                            content: "Embeddings are learned through training processes where neural networks map input data to fixed-size vectors. The key insight is that similar items should have similar vector representations, enabling mathematical operations to capture semantic relationships."
                        },
                        {
                            title: "Types and Applications",
                            content: "Word embeddings represent words as vectors, image embeddings capture visual features, and user embeddings model preferences. These representations enable similarity search, clustering, recommendation systems, and transfer learning across different tasks."
                        }
                    ],
                    example: {
                        title: "Word Embeddings in Search",
                        content: "When you search for 'puppy', systems using word embeddings can also return results for 'dog', 'canine', or 'pet' because these words have similar vector representations, even though they don't share exact text matches."
                    },
                    keyPoints: [
                        "Convert discrete data to continuous vectors",
                        "Capture semantic similarity mathematically",
                        "Enable efficient similarity search and clustering",
                        "Foundation for many modern AI applications"
                    ]
                }
            },
            {
                number: 11,
                title: "Feature Engineering",
                icon: "üîß",
                stage: "core",
                description: "Designing informative features significantly improving model performance",
                lesson: {
                    subtitle: "Crafting Better Input Data",
                    sections: [
                        {
                            title: "What is Feature Engineering?",
                            content: "Feature engineering is the process of selecting, modifying, or creating new features (input variables) from raw data to improve machine learning model performance. It's often considered more art than science, requiring domain expertise and creativity."
                        },
                        {
                            title: "Common Techniques",
                            content: "Techniques include normalization (scaling features to similar ranges), encoding categorical variables, creating polynomial features, extracting features from dates/times, handling missing values, and dimensionality reduction to remove irrelevant features."
                        },
                        {
                            title: "Impact on Model Performance",
                            content: "Good feature engineering can dramatically improve model accuracy, even with simple algorithms. It helps models learn more effectively by providing relevant, clean, and well-structured information that highlights important patterns in the data."
                        }
                    ],
                    example: {
                        title: "Predicting House Prices",
                        content: "Instead of just using raw features like 'year built', feature engineering might create 'house age', 'years since renovation', or 'price per square foot'. These derived features often provide more meaningful information for the prediction task."
                    },
                    keyPoints: [
                        "Transforms raw data into useful features",
                        "Requires domain knowledge and creativity",
                        "Can significantly improve model performance",
                        "Less critical with deep learning but still important"
                    ]
                }
            },
            {
                number: 12,
                title: "Vector Search",
                icon: "üîç",
                stage: "core",
                description: "Finds similar items using dense vector embeddings",
                lesson: {
                    subtitle: "Semantic Search Through Vector Space",
                    sections: [
                        {
                            title: "Vector Search Fundamentals",
                            content: "Vector search uses embeddings to find similar items by measuring distances or similarities between vectors in high-dimensional space. Unlike traditional keyword search, vector search can find semantically similar content even when exact words don't match."
                        },
                        {
                            title: "Distance Metrics",
                            content: "Common distance metrics include cosine similarity (measuring angle between vectors), Euclidean distance (straight-line distance), and dot product similarity. Each metric captures different aspects of similarity and is chosen based on the specific use case."
                        },
                        {
                            title: "Scalability and Optimization",
                            content: "For large datasets, exact vector search becomes computationally expensive. Approximate methods like LSH (Locality-Sensitive Hashing), HNSW (Hierarchical Navigable Small World), and specialized vector databases enable fast similarity search at scale."
                        }
                    ],
                    example: {
                        title: "Recommendation Systems",
                        content: "Spotify uses vector search to recommend music by converting songs into embeddings that capture musical features like genre, tempo, and mood. When you like a song, the system finds other songs with similar vectors to recommend."
                    },
                    keyPoints: [
                        "Enables semantic similarity search",
                        "Works with high-dimensional vector spaces",
                        "Requires efficient algorithms for scale",
                        "Powers recommendation and retrieval systems"
                    ]
                }
            },
            
            // MODERN AI MODELS (13-18)
            {
                number: 13,
                title: "LLM",
                icon: "üåä",
                stage: "modern",
                description: "Generates human-like text using massive pre-training data",
                lesson: {
                    subtitle: "Large Language Models Revolution",
                    sections: [
                        {
                            title: "What are Large Language Models?",
                            content: "Large Language Models (LLMs) are AI systems trained on vast amounts of text data to understand and generate human-like language. They use transformer architecture and contain billions of parameters, enabling them to perform various language tasks with remarkable capability."
                        },
                        {
                            title: "Training Process",
                            content: "LLMs are trained in two main phases: pre-training on massive text corpora where they learn language patterns, and fine-tuning on specific tasks or with human feedback to improve performance and alignment with human preferences."
                        },
                        {
                            title: "Capabilities and Applications",
                            content: "Modern LLMs can engage in conversations, write code, analyze documents, translate languages, summarize content, and reason through complex problems. They exhibit emergent behaviors not explicitly trained for, showing a form of general intelligence."
                        }
                    ],
                    example: {
                        title: "ChatGPT and Similar Systems",
                        content: "ChatGPT demonstrates LLM capabilities by maintaining coherent conversations, answering questions across diverse topics, helping with coding problems, and assisting with creative writing - all through natural language interaction."
                    },
                    keyPoints: [
                        "Trained on massive text datasets",
                        "Can perform diverse language tasks",
                        "Show emergent reasoning capabilities",
                        "Foundation for conversational AI"
                    ]
                }
            },
            {
                number: 14,
                title: "Small Language Models (SLMs)",
                icon: "üì±",  
                stage: "modern",
                description: "Compact AI models optimized for edge devices and IoT applications",
                lesson: {
                    subtitle: "Efficient AI for Resource-Constrained Environments",
                    sections: [
                        {
                            title: "What are Small Language Models?",
                            content: "Small Language Models (SLMs) are lightweight versions of large language models designed to operate efficiently on resource-constrained environments such as smartphones, IoT devices, and edge computing systems. Typically having millions to a few billion parameters (compared to hundreds of billions in LLMs), SLMs balance performance with computational efficiency."
                        },
                        {
                            title: "Key Advantages",
                            content: "SLMs offer several benefits including lower computational requirements, faster inference times, reduced energy consumption, on-device processing for enhanced privacy, and the ability to run offline without internet connectivity. They enable AI capabilities on consumer devices like smartphones, tablets, and embedded systems."
                        },
                        {
                            title: "Applications and Examples",
                            content: "SLMs are ideal for IoT sensor networks, smart home devices, mobile applications, predictive maintenance on industrial equipment, and real-time edge computing scenarios. Popular examples include Microsoft's Phi models (3.8B parameters), Google's Gemma 2B, and various optimized versions of larger models."
                        }
                    ],
                    example: {
                        title: "Smart Home Assistant",
                        content: "A smart thermostat uses an SLM to process voice commands locally, learning user preferences and adjusting temperature settings without sending data to the cloud. The model runs on the device's microprocessor, providing instant responses while maintaining privacy and working even when internet connectivity is poor."
                    },
                    keyPoints: [
                        "Designed for resource-constrained environments",
                        "Enable on-device AI processing and privacy",
                        "Lower energy consumption and faster inference",
                        "Perfect for IoT, mobile, and edge computing applications"
                    ]
                }
            },
            {
                number: 15,
                title: "Open Source LLMs",
                icon: "üîì",
                stage: "modern",
                description: "Freely available large language models for customization and research",
                lesson: {
                    subtitle: "Democratizing Access to Advanced AI Models",
                    sections: [
                        {
                            title: "Understanding Open Source LLMs",
                            content: "Open Source Large Language Models are AI models whose code, architecture, and often training data are publicly available for anyone to use, modify, and distribute. Unlike proprietary models from companies like OpenAI, these models provide transparency, customization opportunities, and freedom from vendor lock-in."
                        },
                        {
                            title: "Popular Open Source Models",
                            content: "Leading open source LLMs include Meta's Llama series (3.1 8B, 70B, 405B), Mistral AI's models (7B, 8x7B Mixtral), Google's Gemma family (2B, 7B, 27B), Alibaba's Qwen models, and Microsoft's Phi series. Each offers different strengths in terms of size, capabilities, and licensing terms."
                        },
                        {
                            title: "Benefits and Considerations",
                            content: "Open source LLMs offer advantages including cost control, data privacy, customization flexibility, and independence from third-party services. However, they require technical expertise to deploy and maintain, and may lack some features of cutting-edge proprietary models. They're ideal for research, custom applications, and organizations with specific privacy or control requirements."
                        }
                    ],
                    example: {
                        title: "Enterprise Custom Assistant",
                        content: "A financial services company uses Llama 3.1 70B to build a custom AI assistant for their internal operations. They fine-tune the model on their proprietary financial data, deploy it on their own infrastructure for data security, and customize it to understand industry-specific terminology and regulations."
                    },
                    keyPoints: [
                        "Transparent, customizable, and vendor-independent",
                        "Include Llama, Mistral, Gemma, Qwen, and Phi families",
                        "Offer cost control and data privacy benefits",
                        "Require technical expertise but provide full control"
                    ]
                }
            },
            {
                number: 16,
                title: "Generative Models",
                icon: "üé®",
                stage: "modern",
                description: "Creating new data samples using learned distributions",
                lesson: {
                    subtitle: "AI That Creates Original Content",
                    sections: [
                        {
                            title: "Understanding Generative Models",
                            content: "Generative models are AI systems that learn to create new data that resembles the training data. Unlike discriminative models that classify or predict, generative models understand the underlying patterns in data well enough to produce realistic new examples."
                        },
                        {
                            title: "Types of Generative Models",
                            content: "Major types include Generative Adversarial Networks (GANs) where two networks compete to create realistic data, Variational Autoencoders (VAEs) that learn compressed representations, and autoregressive models that generate data sequentially."
                        },
                        {
                            title: "How They Work",
                            content: "Generative models learn the probability distribution of the training data. They can then sample from this learned distribution to create new, similar data. The key is learning not just what the data looks like, but understanding the underlying structure and patterns."
                        }
                    ],
                    example: {
                        title: "AI Art Generation",
                        content: "Tools like DALL-E and Midjourney use generative models to create original artwork from text descriptions. They've learned patterns from millions of images and can combine concepts in novel ways, creating art that has never existed before but looks realistic."
                    },
                    keyPoints: [
                        "Creates new, original content",
                        "Learns underlying data distributions",
                        "Applications in art, music, text, and synthetic data",
                        "Includes GANs, VAEs, and diffusion models"
                    ]
                }
            },
            {
                number: 17,
                title: "Multimodal Models",
                icon: "üé≠",
                stage: "modern",
                description: "Processes and generates across multiple data types",
                lesson: {
                    subtitle: "AI That Understands Multiple Modalities",
                    sections: [
                        {
                            title: "What are Multimodal Models?",
                            content: "Multimodal models can process and understand multiple types of data simultaneously, such as text, images, audio, and video. They bridge different modalities to create more comprehensive AI systems that can understand the world more like humans do."
                        },
                        {
                            title: "Cross-Modal Understanding",
                            content: "These models learn relationships between different modalities. For example, they can understand that a picture of a dog relates to the word 'dog' and the sound of barking. This cross-modal understanding enables more sophisticated AI applications."
                        },
                        {
                            title: "Architecture and Training",
                            content: "Multimodal models often use shared representations where different modalities are mapped to a common embedding space. Training involves learning from paired data across modalities, such as images with captions or videos with audio descriptions."
                        }
                    ],
                    example: {
                        title: "Image Captioning and Visual Question Answering",
                        content: "GPT-4 with vision can look at an image and answer questions about it or generate detailed descriptions. It combines computer vision capabilities with language understanding to bridge the gap between visual and textual information."
                    },
                    keyPoints: [
                        "Processes multiple data types simultaneously",
                        "Learns relationships between modalities",
                        "Enables more natural human-AI interaction",
                        "Applications in robotics, content creation, and accessibility"
                    ]
                }
            },
            {
                number: 18,
                title: "AI Agents",
                icon: "ü§ñ",
                stage: "modern",
                description: "Autonomous systems that perceive, decide, and act",
                lesson: {
                    subtitle: "AI That Takes Action",
                    sections: [
                        {
                            title: "Understanding AI Agents",
                            content: "AI agents are autonomous systems that can perceive their environment, make decisions, and take actions to achieve specific goals. Unlike passive AI models that only respond to inputs, agents actively interact with their environment and can adapt their behavior based on outcomes."
                        },
                        {
                            title: "Agent Components",
                            content: "AI agents typically include sensors (to perceive the environment), actuators (to take actions), a knowledge base (stored information), and decision-making algorithms (to choose appropriate actions). They also have goals or objectives that guide their behavior."
                        },
                        {
                            title: "Types of AI Agents",
                            content: "Agents can be reactive (responding to immediate stimuli), deliberative (planning ahead), or hybrid (combining both approaches). They can operate in single-agent environments or multi-agent systems where multiple agents interact and potentially collaborate or compete."
                        }
                    ],
                    example: {
                        title: "Virtual Assistants",
                        content: "Siri, Alexa, and Google Assistant are AI agents that perceive voice commands, understand intent, access knowledge bases, make decisions about how to respond, and take actions like setting reminders, playing music, or controlling smart home devices."
                    },
                    keyPoints: [
                        "Autonomous systems that act in environments",
                        "Combine perception, decision-making, and action",
                        "Can adapt behavior based on outcomes",
                        "Applications in robotics, virtual assistants, and automation"
                    ]
                }
            },
            
            // AI TECHNIQUES & OPTIMIZATION (19-23)
            {
                number: 19,
                title: "LoRA & PEFT",
                icon: "üéØ",
                stage: "techniques",
                description: "Parameter-efficient fine-tuning techniques for large models",
                lesson: {
                    subtitle: "Efficient Model Customization Without Full Retraining",
                    sections: [
                        {
                            title: "What is PEFT and LoRA?",
                            content: "Parameter-Efficient Fine-Tuning (PEFT) techniques enable efficient adaptation of large pre-trained models by fine-tuning only a small subset of parameters instead of the entire model. Low-Rank Adaptation (LoRA) is the most popular PEFT method, which injects trainable low-rank matrices into model layers while keeping original weights frozen."
                        },
                        {
                            title: "How LoRA Works",
                            content: "LoRA decomposes weight update matrices into two smaller matrices (A and B) that when multiplied approximate the full weight changes. Instead of updating millions of parameters, LoRA trains only these smaller matrices, reducing trainable parameters by up to 10,000x while maintaining performance comparable to full fine-tuning."
                        },
                        {
                            title: "Benefits and Variations",
                            content: "PEFT methods offer dramatic reductions in computational requirements, memory usage, and training time. Popular variations include QLoRA (quantized LoRA for even greater efficiency), AdaLoRA (adaptive rank selection), and task-specific adapters. These techniques make fine-tuning accessible on consumer hardware and enable multiple task-specific models from one base model."
                        }
                    ],
                    example: {
                        title: "Domain-Specific Chatbot",
                        content: "A medical research team uses LoRA to fine-tune Llama 2 7B for medical question-answering. Instead of training all 7 billion parameters, they train only 4 million LoRA parameters on medical literature, achieving specialized performance while using a single consumer GPU and completing training in hours rather than weeks."
                    },
                    keyPoints: [
                        "Fine-tune large models with minimal computational resources",
                        "LoRA reduces trainable parameters by up to 10,000x",
                        "Maintains performance comparable to full fine-tuning",
                        "Enables multiple specialized models from one base model"
                    ]
                }
            },
            {
                number: 20,
                title: "Fine Tuning Models",
                icon: "‚öôÔ∏è",
                stage: "techniques",
                description: "Customizes pre-trained models for domain-specific tasks",
                lesson: {
                    subtitle: "Adapting Pre-trained Models",
                    sections: [
                        {
                            title: "Understanding Fine-Tuning",
                            content: "Fine-tuning is the process of taking a pre-trained model and adapting it for a specific task or domain. Instead of training from scratch, fine-tuning leverages the general knowledge already learned by the model and specializes it for particular applications."
                        },
                        {
                            title: "The Fine-Tuning Process",
                            content: "Fine-tuning typically involves taking a model trained on a large, general dataset and continuing training on a smaller, task-specific dataset. The learning rate is usually lower than initial training to preserve the general knowledge while adapting to the new task."
                        },
                        {
                            title: "Benefits and Applications",
                            content: "Fine-tuning is more efficient than training from scratch, requires less data, and often achieves better performance. It's commonly used in computer vision (adapting image classifiers), NLP (customizing language models), and many other domains where pre-trained models exist."
                        }
                    ],
                    example: {
                        title: "Medical Image Analysis",
                        content: "A model pre-trained on general images can be fine-tuned on medical scans to detect specific diseases. The pre-trained model already understands basic visual features, so fine-tuning teaches it to recognize medical-specific patterns like tumors or fractures."
                    },
                    keyPoints: [
                        "Adapts pre-trained models to specific tasks",
                        "More efficient than training from scratch",
                        "Requires less task-specific data",
                        "Widely used in computer vision and NLP"
                    ]
                }
            },
            {
                number: 21,
                title: "Prompt Engineering",
                icon: "‚úçÔ∏è",
                stage: "techniques",
                description: "Crafting effective inputs to guide generative model outputs",
                lesson: {
                    subtitle: "The Art of Communicating with AI",
                    sections: [
                        {
                            title: "What is Prompt Engineering?",
                            content: "Prompt engineering is the practice of designing and optimizing input prompts to get desired outputs from AI models, particularly large language models. It's the interface between human intent and AI capability, requiring both technical understanding and creative communication."
                        },
                        {
                            title: "Effective Prompt Strategies",
                            content: "Good prompts are clear, specific, and provide context. Techniques include few-shot learning (providing examples), chain-of-thought prompting (asking for step-by-step reasoning), and role-playing (asking the AI to adopt a specific persona or expertise)."
                        },
                        {
                            title: "Advanced Techniques",
                            content: "Advanced prompt engineering includes prompt chaining (breaking complex tasks into steps), temperature control (adjusting randomness), and prompt optimization (systematically testing and refining prompts for better performance)."
                        }
                    ],
                    example: {
                        title: "Code Generation",
                        content: "Instead of asking 'write code for a website', an effective prompt might be: 'Create a responsive HTML/CSS webpage for a bakery with a header, menu section, and contact form. Use modern CSS Grid layout and include hover effects on menu items.'"
                    },
                    keyPoints: [
                        "Critical skill for working with LLMs",
                        "Combines technical knowledge with communication",
                        "Can dramatically improve AI output quality",
                        "Involves iterative testing and refinement"
                    ]
                }
            },
            {
                number: 22,
                title: "RAG",
                icon: "üîç",
                stage: "techniques",
                description: "Retrieval-Augmented Generation combines LLMs with external knowledge",
                lesson: {
                    subtitle: "Enhancing LLMs with External Knowledge",
                    sections: [
                        {
                            title: "What is RAG?",
                            content: "Retrieval-Augmented Generation (RAG) is a technique that enhances large language models by allowing them to retrieve and incorporate relevant information from external knowledge sources before generating responses. Instead of relying solely on training data, RAG systems can access up-to-date, domain-specific information."
                        },
                        {
                            title: "How RAG Works",
                            content: "RAG operates in two phases: retrieval and generation. First, when a user asks a question, the system searches external databases or documents for relevant information. This retrieved content is then combined with the original query and fed to the LLM, which generates a response incorporating both its training knowledge and the fresh external information."
                        },
                        {
                            title: "Benefits and Applications",
                            content: "RAG addresses key limitations of LLMs including hallucination, outdated information, and lack of domain-specific knowledge. It's particularly valuable for enterprise applications where companies need AI systems that can access proprietary data, current information, and specialized knowledge without retraining the entire model."
                        }
                    ],
                    example: {
                        title: "Customer Support Chatbot",
                        content: "A company's customer service chatbot uses RAG to access the latest product documentation, FAQ databases, and recent policy updates. When a customer asks about warranty terms, the system retrieves current warranty information and generates an accurate, up-to-date response rather than relying on potentially outdated training data."
                    },
                    keyPoints: [
                        "Combines LLM capabilities with external knowledge retrieval",
                        "Reduces hallucination and provides more accurate responses",
                        "Enables access to current and domain-specific information",
                        "Cost-effective alternative to fine-tuning or retraining models"
                    ]
                }
            },
            {
                number: 23,
                title: "Model Evaluation",
                icon: "üìà",
                stage: "techniques",
                description: "Assessing predictive performance using validation techniques",
                lesson: {
                    subtitle: "Measuring AI Model Performance",
                    sections: [
                        {
                            title: "Why Model Evaluation Matters",
                            content: "Model evaluation is crucial for understanding how well an AI system performs on unseen data. It helps identify overfitting, compare different models, and ensure the model meets performance requirements before deployment in real-world applications."
                        },
                        {
                            title: "Evaluation Metrics",
                            content: "Different tasks require different metrics: accuracy and F1-score for classification, RMSE and MAE for regression, BLEU scores for translation, and perplexity for language models. The choice of metric depends on the specific problem and business requirements."
                        },
                        {
                            title: "Validation Techniques",
                            content: "Cross-validation splits data into multiple folds for robust evaluation. Hold-out validation reserves a test set that's never seen during training. Time-series validation respects temporal order. These techniques help estimate how the model will perform on new data."
                        }
                    ],
                    example: {
                        title: "Medical AI Validation",
                        content: "Before deploying an AI system to diagnose medical conditions, it must be rigorously evaluated on diverse patient populations, with metrics like sensitivity (catching true positives) and specificity (avoiding false positives) carefully measured."
                    },
                    keyPoints: [
                        "Essential for understanding model performance",
                        "Uses different metrics for different tasks",
                        "Includes techniques like cross-validation",
                        "Critical for deployment decisions"
                    ]
                }
            },
            
            // INFRASTRUCTURE & OPERATIONS (24-26)
            {
                number: 24,
                title: "LLMOps",
                icon: "‚öôÔ∏è",
                stage: "infrastructure",
                description: "Large Language Model Operations for production deployment",
                lesson: {
                    subtitle: "Managing LLMs in Production Environments",
                    sections: [
                        {
                            title: "What is LLMOps?",
                            content: "Large Language Model Operations (LLMOps) is a specialized set of practices, tools, and methodologies for managing the complete lifecycle of large language models in production environments. It extends MLOps principles to address the unique challenges of deploying, monitoring, and maintaining LLMs at scale."
                        },
                        {
                            title: "Key Components",
                            content: "LLMOps encompasses data preparation and management, model fine-tuning and optimization, deployment orchestration, performance monitoring, cost management, and security compliance. It involves collaboration between data scientists, ML engineers, DevOps teams, and IT professionals to ensure reliable LLM operations."
                        },
                        {
                            title: "Unique Challenges",
                            content: "LLMs present distinct operational challenges including massive computational requirements, high inference costs, complex evaluation metrics, prompt engineering needs, and rapid model evolution. LLMOps addresses these through specialized tools for resource optimization, cost monitoring, automated testing, and version management."
                        }
                    ],
                    example: {
                        title: "Enterprise AI Platform",
                        content: "A financial services company uses LLMOps to manage their customer service AI. The platform handles model deployment across multiple regions, monitors response quality and latency, manages prompt versions, tracks costs per interaction, ensures compliance with financial regulations, and automatically scales resources based on demand."
                    },
                    keyPoints: [
                        "Specialized MLOps practices for large language models",
                        "Manages entire LLM lifecycle from development to production",
                        "Addresses unique challenges of scale, cost, and complexity",
                        "Enables reliable, efficient deployment of LLM applications"
                    ]
                }
            },
            {
                number: 25,
                title: "AI Infrastructure",
                icon: "‚òÅÔ∏è",
                stage: "infrastructure",
                description: "Deploying scalable systems to support AI operations",
                lesson: {
                    subtitle: "Building Systems That Support AI",
                    sections: [
                        {
                            title: "AI Infrastructure Components",
                            content: "AI infrastructure includes the hardware (GPUs, TPUs, CPUs), software frameworks (TensorFlow, PyTorch), data storage systems, model serving platforms, and monitoring tools needed to develop, train, and deploy AI models at scale."
                        },
                        {
                            title: "Scalability Challenges",
                            content: "AI workloads require significant computational resources and can be unpredictable. Infrastructure must handle training large models, serving predictions at low latency, managing large datasets, and scaling up or down based on demand."
                        },
                        {
                            title: "MLOps and Model Lifecycle",
                            content: "Modern AI infrastructure includes MLOps practices for versioning models, automating training pipelines, continuous integration/deployment, model monitoring, and managing the entire machine learning lifecycle from development to production."
                        }
                    ],
                    example: {
                        title: "Cloud AI Services",
                        content: "Companies like Google Cloud, AWS, and Azure provide AI infrastructure including pre-trained models, training environments, model serving endpoints, and monitoring tools, allowing organizations to focus on their specific AI applications rather than infrastructure management."
                    },
                    keyPoints: [
                        "Combines hardware, software, and operational practices",
                        "Must handle large-scale, resource-intensive workloads",
                        "Includes MLOps for model lifecycle management",
                        "Often leverages cloud platforms for scalability"
                    ]
                }
            },
            {
                number: 26,
                title: "MCP",
                icon: "üîå",
                stage: "infrastructure",
                description: "Model Context Protocol standardizes AI-tool connections",
                lesson: {
                    subtitle: "The Universal Connector for AI Systems",
                    sections: [
                        {
                            title: "Understanding MCP",
                            content: "Model Context Protocol (MCP) is an open standard developed by Anthropic that provides a universal method for connecting AI assistants to external tools, data sources, and systems. Think of MCP as 'USB-C for AI' - it standardizes how AI models interact with various external resources without requiring custom integrations for each tool."
                        },
                        {
                            title: "How MCP Works",
                            content: "MCP creates a standardized communication layer between AI models (clients) and external services (servers). Instead of building separate connectors for each data source, developers can create MCP servers that expose specific capabilities through a unified protocol. AI models can then discover and interact with these tools dynamically."
                        },
                        {
                            title: "Benefits and Architecture",
                            content: "MCP enables secure, two-way communication between AI systems and external tools. It supports dynamic discovery of available tools, standardized authentication, and real-time data access. This architecture makes it easier to build AI agents that can interact with multiple systems seamlessly while maintaining security and scalability."
                        }
                    ],
                    example: {
                        title: "AI Development Assistant",
                        content: "A coding AI assistant uses MCP to connect with GitHub (for code repositories), Slack (for team communication), Google Drive (for documentation), and local development tools. Through a single protocol, the AI can read code, suggest improvements, create pull requests, notify team members, and update documentation - all without requiring separate API integrations."
                    },
                    keyPoints: [
                        "Standardizes AI-tool communication across platforms",
                        "Reduces development complexity for AI integrations",
                        "Enables dynamic discovery and interaction with tools",
                        "Supports secure, bidirectional data exchange"
                    ]
                }
            },
            
            // ADVANCED & FUTURE CONCEPTS (27-29)
            {
                number: 27,
                title: "AI Evaluations",
                icon: "üìä",
                stage: "advanced",
                description: "Methods to assess AI model performance, safety, and reliability",
                lesson: {
                    subtitle: "Measuring AI System Quality and Safety",
                    sections: [
                        {
                            title: "What are AI Evaluations?",
                            content: "AI evaluations are systematic methods for assessing the performance, safety, reliability, and ethical behavior of AI systems. Unlike traditional software testing, AI evaluations must account for probabilistic outputs, edge cases, bias detection, and alignment with human values and intentions."
                        },
                        {
                            title: "Types of Evaluations",
                            content: "AI evaluations include capability assessments (measuring what the model can do), safety evaluations (identifying potential harms), robustness testing (performance under adversarial conditions), bias and fairness audits, alignment evaluation (following intended behavior), and red team exercises to find vulnerabilities."
                        },
                        {
                            title: "Modern Evaluation Challenges",
                            content: "Evaluating modern AI systems requires new methodologies due to their complexity and general-purpose nature. This includes developing appropriate benchmarks, handling subjective tasks, measuring emergent behaviors, assessing real-world performance, and ensuring evaluations remain relevant as capabilities rapidly advance."
                        }
                    ],
                    example: {
                        title: "LLM Safety Evaluation",
                        content: "Before deploying a customer-facing chatbot, a company conducts comprehensive AI evaluations including accuracy testing on domain-specific questions, bias detection across demographic groups, safety testing for harmful content generation, robustness testing with adversarial prompts, and alignment verification to ensure helpful, harmless responses."
                    },
                    keyPoints: [
                        "Systematic assessment of AI performance and safety",
                        "Includes capability, safety, bias, and alignment testing",
                        "Critical for responsible AI deployment",
                        "Evolving field requiring new methodologies for advanced AI"
                    ]
                }
            },
            {
                number: 28,
                title: "AGI",
                icon: "üåü",
                stage: "advanced",
                description: "Artificial General Intelligence with human-level cognitive abilities",
                lesson: {
                    subtitle: "The Holy Grail of Artificial Intelligence",
                    sections: [
                        {
                            title: "What is Artificial General Intelligence?",
                            content: "Artificial General Intelligence (AGI) refers to a theoretical type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks. Unlike narrow AI systems designed for specific tasks, AGI would possess the ability to understand, learn, and apply knowledge across diverse domains with human-like flexibility, creativity, and reasoning."
                        },
                        {
                            title: "Key Characteristics",
                            content: "AGI systems would exhibit autonomous self-control, self-understanding, and the ability to learn new skills without task-specific programming. They would demonstrate general problem-solving capabilities, adapt to unfamiliar situations, transfer knowledge between domains, and potentially possess consciousness and self-awareness. AGI represents 'strong AI' as opposed to the 'narrow AI' we have today."
                        },
                        {
                            title: "Current Status and Challenges",
                            content: "AGI remains theoretical and highly debated among researchers. While some argue that current large language models show early signs of AGI-level capabilities, most experts believe we are still decades away from true AGI. Achieving AGI requires breakthroughs in understanding consciousness, developing new architectures, and solving complex challenges in reasoning, creativity, and general intelligence."
                        }
                    ],
                    example: {
                        title: "Universal Problem Solver",
                        content: "An AGI system could seamlessly transition from diagnosing medical conditions to composing music, writing code, conducting scientific research, and engaging in philosophical discussions - all without specific training for each task. It would understand context, learn from experience, and apply knowledge creatively across any domain, much like a highly intelligent human."
                    },
                    keyPoints: [
                        "Theoretical AI matching human cognitive abilities across all domains",
                        "Exhibits general intelligence, not task-specific capabilities",
                        "Would possess learning, reasoning, and creative problem-solving",
                        "Remains a research goal with significant technical challenges"
                    ]
                }
            },
            {
                number: 29,
                title: "LLM OS",
                icon: "üíª",
                stage: "advanced",
                description: "Large Language Model Operating Systems for natural language computing",
                lesson: {
                    subtitle: "AI-Powered Operating Systems of the Future",
                    sections: [
                        {
                            title: "Understanding LLM Operating Systems",
                            content: "LLM OS represents a paradigm shift where large language models serve as the core interface and intelligence layer of operating systems. Instead of traditional graphical interfaces and predefined commands, users interact with their computers through natural language, with the LLM interpreting intentions and executing complex tasks autonomously."
                        },
                        {
                            title: "How LLM OS Works",
                            content: "An LLM OS integrates language models directly into the system architecture, either at the kernel level or application level. The LLM acts as an intelligent intermediary between users and system resources, managing memory, executing commands, coordinating applications, and providing context-aware assistance. It can understand complex requests like 'organize my desktop and find that presentation I was working on last week.'"
                        },
                        {
                            title: "Current Examples and Future Vision",
                            content: "Early implementations include AIOS (AI Operating System), MemGPT for context management, and various research projects exploring LLM integration. The vision extends to systems that can automate routine tasks, provide intelligent troubleshooting, manage files through natural language, and offer personalized computing experiences that adapt to user needs and preferences."
                        }
                    ],
                    example: {
                        title: "Intelligent Desktop Assistant",
                        content: "Instead of manually organizing files, a user could say 'Create a project folder for my quarterly report, move all related documents there, and schedule a reminder to review it tomorrow.' The LLM OS would understand the context, create the folder structure, intelligently identify and move relevant files, and integrate with calendar systems - all through natural language interaction."
                    },
                    keyPoints: [
                        "Integrates LLMs as core OS intelligence layer",
                        "Enables natural language computing interfaces",
                        "Automates complex system tasks and workflows",
                        "Represents future of human-computer interaction"
                    ]
                }
            }
        ];

        let currentConceptIndex = -1;

        function getStageClass(stage) {
            return `stage-${stage}`;
        }

        function getStageLabel(stage) {
            const labels = {
                foundation: "Foundation",
                core: "Core AI",
                modern: "Modern",
                techniques: "Techniques", 
                infrastructure: "Infrastructure",
                advanced: "Advanced"
            };
            return labels[stage] || stage;
        }

        // Generate concept cards
        function generateConceptCards() {
            const grid = document.getElementById('conceptGrid');
            
            concepts.forEach(concept => {
                const card = document.createElement('div');
                card.className = 'concept-card';
                card.onclick = () => openModal(concept);
                
                card.innerHTML = `
                    <div class="concept-number">${concept.number}</div>
                    <div class="stage-indicator ${getStageClass(concept.stage)}">${getStageLabel(concept.stage)}</div>
                    <div class="concept-title">${concept.title}</div>
                    <div class="concept-icon">${concept.icon}</div>
                    <div class="concept-description">${concept.description}</div>
                `;
                
                grid.appendChild(card);
            });
        }

        // Open modal with lesson content
        function openModal(concept) {
            const modal = document.getElementById('conceptModal');
            const modalTitle = document.getElementById('modalTitle');
            const modalSubtitle = document.getElementById('modalSubtitle');
            const modalBody = document.getElementById('modalBody');
            
            // Find and set current concept index
            currentConceptIndex = concepts.findIndex(c => c.number === concept.number);
            
            modalTitle.textContent = concept.title;
            modalSubtitle.textContent = concept.lesson.subtitle;
            
            // Update navigation
            updateNavigation();
            
            let bodyContent = '';
            
            // Add lesson sections
            concept.lesson.sections.forEach(section => {
                bodyContent += `
                    <div class="lesson-section">
                        <h3>${section.title}</h3>
                        <p>${section.content}</p>
                    </div>
                `;
            });
            
            // Add example
            if (concept.lesson.example) {
                bodyContent += `
                    <div class="example-box">
                        <h4>${concept.lesson.example.title}</h4>
                        <p>${concept.lesson.example.content}</p>
                    </div>
                `;
            }
            
            // Add key points
            if (concept.lesson.keyPoints) {
                bodyContent += `
                    <div class="key-points">
                        <h4>Key Takeaways</h4>
                        <ul>
                            ${concept.lesson.keyPoints.map(point => `<li>${point}</li>`).join('')}
                        </ul>
                    </div>
                `;
            }
            
            modalBody.innerHTML = bodyContent;
            modal.style.display = 'block';
            document.body.style.overflow = 'hidden';
            
            // Show keyboard controls - with slight delay to ensure modal is rendered
            setTimeout(() => {
                showKeyboardControls();
            }, 100);
        }

        // Update navigation buttons and position
        function updateNavigation() {
            const prevBtn = document.getElementById('prevConcept');
            const nextBtn = document.getElementById('nextConcept');
            const position = document.getElementById('conceptPosition');
            
            // Update position indicator
            position.textContent = `${currentConceptIndex + 1} of ${concepts.length}`;
            
            // Update previous button
            if (currentConceptIndex <= 0) {
                prevBtn.classList.add('disabled');
            } else {
                prevBtn.classList.remove('disabled');
            }
            
            // Update next button
            if (currentConceptIndex >= concepts.length - 1) {
                nextBtn.classList.add('disabled');
            } else {
                nextBtn.classList.remove('disabled');
            }
        }

        // Navigate to previous concept
        function navigatePrevious() {
            if (currentConceptIndex > 0) {
                currentConceptIndex--;
                openModal(concepts[currentConceptIndex]);
            }
        }

        // Navigate to next concept
        function navigateNext() {
            if (currentConceptIndex < concepts.length - 1) {
                currentConceptIndex++;
                openModal(concepts[currentConceptIndex]);
            }
        }

        // Show keyboard controls overlay
        function showKeyboardControls() {
            const controls = document.getElementById('keyboardControlsModal');
            controls.classList.add('show');
            
            // Hide after 5 seconds (increased duration)
            setTimeout(() => {
                if (controls.classList.contains('show')) {
                    controls.classList.remove('show');
                }
            }, 5000);
        }

        // Force show controls for testing
        function forceShowControls() {
            const controls = document.getElementById('keyboardControls');
            if (controls) {
                controls.style.display = 'block';
                controls.classList.add('show');
            }
            // Also try the modal version
            const modalControls = document.getElementById('keyboardControlsModal');
            if (modalControls) {
                modalControls.style.display = 'block';
                modalControls.classList.add('show');
            }
        }

        // Close modal
        function closeModal() {
            const modal = document.getElementById('conceptModal');
            modal.style.display = 'none';
            document.body.style.overflow = 'auto';
            currentConceptIndex = -1;
            
            // Hide both keyboard controls versions
            const controls = document.getElementById('keyboardControls');
            if (controls) controls.classList.remove('show');
            
            const modalControls = document.getElementById('keyboardControlsModal');
            if (modalControls) modalControls.classList.remove('show');
        }

        // Event listeners
        document.addEventListener('DOMContentLoaded', function() {
            generateConceptCards();
            
            // Close modal when clicking X
            document.querySelector('.close').onclick = closeModal;
            
            // Navigation button events
            document.getElementById('prevConcept').onclick = function() {
                if (!this.classList.contains('disabled')) {
                    navigatePrevious();
                }
            };
            
            document.getElementById('nextConcept').onclick = function() {
                if (!this.classList.contains('disabled')) {
                    navigateNext();
                }
            };
            
            // Close modal when clicking outside
            window.onclick = function(event) {
                const modal = document.getElementById('conceptModal');
                if (event.target === modal) {
                    closeModal();
                }
            };
            
            // Keyboard navigation
            document.addEventListener('keydown', function(event) {
                const modal = document.getElementById('conceptModal');
                if (modal.style.display === 'block') {
                    switch(event.key) {
                        case 'Escape':
                            closeModal();
                            break;
                        case 'ArrowLeft':
                            event.preventDefault();
                            navigatePrevious();
                            break;
                        case 'ArrowRight':
                            event.preventDefault();
                            navigateNext();
                            break;
                    }
                }
            });
            
            // Show keyboard controls when hovering over modal
            const modal = document.getElementById('conceptModal');
            modal.addEventListener('mouseenter', function() {
                if (modal.style.display === 'block') {
                    showKeyboardControls();
                }
            });
        });
    </script>
</body>
</html>
