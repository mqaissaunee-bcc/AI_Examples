<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Machine Learning Algorithms - Interactive Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            line-height: 1.6;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }

        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }

        header p {
            font-size: 1.2em;
            opacity: 0.9;
        }

        .controls {
            background: #f8f9fa;
            padding: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            justify-content: center;
            align-items: center;
            border-bottom: 3px solid #1e3c72;
        }

        .filter-group {
            display: flex;
            flex-direction: column;
            gap: 5px;
        }

        .filter-group label {
            font-weight: 600;
            color: #333;
            font-size: 0.9em;
        }

        select, input[type="text"] {
            padding: 10px 15px;
            border: 2px solid #ddd;
            border-radius: 8px;
            font-size: 1em;
            transition: all 0.3s;
            min-width: 200px;
        }

        select:focus, input[type="text"]:focus {
            outline: none;
            border-color: #1e3c72;
            box-shadow: 0 0 0 3px rgba(30, 60, 114, 0.1);
        }

        .algorithm-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(350px, 1fr));
            gap: 20px;
            padding: 30px;
        }

        .algorithm-card {
            background: white;
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            overflow: hidden;
            transition: all 0.3s;
            border: 2px solid transparent;
            cursor: pointer;
        }

        .algorithm-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 24px rgba(0,0,0,0.15);
            border-color: #1e3c72;
        }

        .algorithm-card:focus {
            outline: 3px solid #1e3c72;
            outline-offset: 2px;
        }

        .card-header {
            background: #8B2635;
            color: white;
            padding: 20px;
            font-weight: bold;
            font-size: 1.3em;
            text-align: center;
        }

        .card-type {
            background: #0E7C7B;
            color: white;
            padding: 8px 15px;
            text-align: center;
            font-size: 0.9em;
            font-weight: 600;
        }

        .card-content {
            padding: 20px;
        }

        .info-section {
            margin-bottom: 15px;
        }

        .info-label {
            font-weight: 700;
            color: #333;
            margin-bottom: 5px;
            font-size: 0.95em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .info-value {
            color: #555;
            padding: 8px 12px;
            background: #f8f9fa;
            border-radius: 6px;
            font-size: 0.95em;
        }

        .pros {
            background: #d4edda;
            border-left: 4px solid #28a745;
        }

        .cons {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
        }

        .use-case {
            background: #d9e8f5;
            border-left: 4px solid #1e3c72;
        }

        .formula {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        .expand-btn {
            width: 100%;
            padding: 12px;
            background: #1e3c72;
            color: white;
            border: none;
            border-radius: 8px;
            font-size: 1em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            margin-top: 15px;
        }

        .expand-btn:hover {
            background: #2a5298;
            transform: scale(1.02);
        }

        .expand-btn:focus {
            outline: 3px solid #ffc107;
            outline-offset: 2px;
        }

        .expanded-content {
            display: none;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 2px dashed #ddd;
            animation: slideDown 0.3s ease;
        }

        .expanded-content.show {
            display: block;
        }

        @keyframes slideDown {
            from {
                opacity: 0;
                transform: translateY(-10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .example-box {
            background: #e7f3ff;
            padding: 15px;
            border-radius: 8px;
            margin-top: 10px;
            border-left: 4px solid #0066cc;
        }

        .example-title {
            font-weight: 700;
            color: #0066cc;
            margin-bottom: 8px;
        }

        .additional-uses {
            list-style: none;
            padding: 0;
        }

        .additional-uses li {
            padding: 8px 12px;
            background: #f8f9fa;
            margin-bottom: 8px;
            border-radius: 6px;
            border-left: 3px solid #28a745;
        }

        .additional-uses li:before {
            content: "‚úì ";
            color: #28a745;
            font-weight: bold;
            margin-right: 8px;
        }

        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0,0,0,0.8);
            z-index: 1000;
            padding: 20px;
            overflow-y: auto;
        }

        .modal.show {
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .modal-content {
            background: white;
            border-radius: 15px;
            max-width: 900px;
            width: 100%;
            max-height: 90vh;
            overflow-y: auto;
            position: relative;
            animation: modalSlideIn 0.3s ease;
        }

        @keyframes modalSlideIn {
            from {
                opacity: 0;
                transform: scale(0.9);
            }
            to {
                opacity: 1;
                transform: scale(1);
            }
        }

        .modal-header {
            background: #8B2635;
            color: white;
            padding: 25px;
            position: sticky;
            top: 0;
            z-index: 10;
        }

        .modal-header h2 {
            font-size: 2em;
            margin-bottom: 5px;
        }

        .modal-body {
            padding: 30px;
        }

        .close-modal {
            position: absolute;
            top: 20px;
            right: 20px;
            background: white;
            color: #8B2635;
            border: none;
            width: 40px;
            height: 40px;
            border-radius: 50%;
            font-size: 1.5em;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s;
        }

        .close-modal:hover {
            transform: rotate(90deg);
            background: #f8d7da;
        }

        .close-modal:focus {
            outline: 3px solid #ffc107;
        }

        .stats-bar {
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            border-bottom: 3px solid #1e3c72;
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            align-items: center;
            gap: 20px;
        }

        .stats-bar span {
            margin: 0;
            font-weight: 600;
            color: #333;
        }

        .helper-btn {
            padding: 10px 20px;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            border: none;
            border-radius: 8px;
            font-size: 0.95em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }

        .helper-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
            background: linear-gradient(135deg, #2a5298 0%, #3a62b8 100%);
        }

        .helper-btn:focus {
            outline: 3px solid #ffc107;
            outline-offset: 2px;
        }

        .no-results {
            text-align: center;
            padding: 60px 20px;
            color: #666;
        }

        .no-results h2 {
            font-size: 2em;
            margin-bottom: 10px;
        }

        .complexity-indicator {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
            margin-top: 5px;
        }

        .complexity-low {
            background: #d4edda;
            color: #155724;
        }

        .complexity-medium {
            background: #fff3cd;
            color: #856404;
        }

        .complexity-high {
            background: #f8d7da;
            color: #721c24;
        }

        footer {
            background: #1e3c72;
            color: white;
            padding: 20px;
            text-align: center;
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 1.8em;
            }

            .algorithm-grid {
                grid-template-columns: 1fr;
                padding: 15px;
            }

            .controls {
                flex-direction: column;
            }

            select, input[type="text"] {
                width: 100%;
            }

            .modal-content {
                margin: 10px;
            }

            .stats-bar {
                flex-direction: column;
                gap: 10px;
            }

            .helper-btn {
                width: 100%;
            }

            .explainer-popup {
                width: 95%;
                margin: 10px;
            }
        }

        .keyboard-hint {
            background: #fff3cd;
            padding: 10px;
            text-align: center;
            font-size: 0.9em;
            color: #856404;
            border-bottom: 2px solid #ffc107;
        }

        .info-icon {
            display: inline-block;
            width: 22px;
            height: 22px;
            background: #1e3c72;
            color: white;
            border-radius: 50%;
            text-align: center;
            line-height: 22px;
            font-size: 0.9em;
            font-weight: bold;
            cursor: pointer;
            margin-left: 8px;
            transition: all 0.3s;
            vertical-align: middle;
        }

        .info-icon:hover {
            background: #2a5298;
            transform: scale(1.1);
        }

        .info-icon:focus {
            outline: 3px solid #ffc107;
            outline-offset: 2px;
        }

        .explainer-popup {
            display: none;
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.3);
            max-width: 700px;
            width: 90%;
            max-height: 80vh;
            overflow-y: auto;
            z-index: 1001;
            animation: popupSlideIn 0.3s ease;
        }

        .explainer-popup.show {
            display: block;
        }

        @keyframes popupSlideIn {
            from {
                opacity: 0;
                transform: translate(-50%, -45%);
            }
            to {
                opacity: 1;
                transform: translate(-50%, -50%);
            }
        }

        .explainer-header {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 20px 25px;
            border-radius: 15px 15px 0 0;
            position: relative;
        }

        .explainer-header h3 {
            font-size: 1.5em;
            margin: 0;
        }

        .explainer-content {
            padding: 25px;
        }

        .learning-type {
            margin-bottom: 25px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
            border-left: 5px solid #1e3c72;
            transition: all 0.3s;
        }

        .learning-type:hover {
            background: #e9ecef;
            transform: translateX(5px);
        }

        .learning-type-title {
            font-size: 1.3em;
            font-weight: bold;
            color: #1e3c72;
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .learning-type-icon {
            font-size: 1.5em;
        }

        .learning-type-description {
            color: #555;
            line-height: 1.6;
            margin-bottom: 12px;
        }

        .learning-type-examples {
            background: white;
            padding: 12px;
            border-radius: 6px;
            font-size: 0.95em;
            color: #666;
        }

        .learning-type-examples strong {
            color: #1e3c72;
        }

        .overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0,0,0,0.6);
            z-index: 1000;
        }

        .overlay.show {
            display: block;
        }

        .close-explainer {
            position: absolute;
            top: 15px;
            right: 15px;
            background: white;
            color: #1e3c72;
            border: none;
            width: 35px;
            height: 35px;
            border-radius: 50%;
            font-size: 1.3em;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s;
        }

        .close-explainer:hover {
            transform: rotate(90deg);
            background: #f8d7da;
        }

        .close-explainer:focus {
            outline: 3px solid #ffc107;
        }

        .learning-type-examples table tr:hover {
            background: #e9ecef !important;
        }

        .learning-type-examples table th,
        .learning-type-examples table td {
            vertical-align: top;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>ü§ñ Machine Learning Algorithms</h1>
            <p>Interactive Guide & Reference</p>
        </header>

        <div class="keyboard-hint">
            üí° Tip: Click the <strong>?</strong> icons and guide buttons for helpful explanations | Tab to navigate, Enter to select, Escape to close
        </div>

        <div class="controls">
            <div class="filter-group">
                <label for="typeFilter">
                    Filter by Type:
                    <span class="info-icon" 
                          onclick="showExplainer()" 
                          onkeypress="handleInfoKeyPress(event)"
                          tabindex="0"
                          role="button"
                          aria-label="Learn about machine learning types">?</span>
                </label>
                <select id="typeFilter" aria-label="Filter algorithms by type">
                    <option value="all">All Types</option>
                    <option value="Supervised">Supervised Learning</option>
                    <option value="Unsupervised">Unsupervised Learning</option>
                    <option value="Dimensionality Reduction">Dimensionality Reduction</option>
                    <option value="Self-supervised">Self-supervised Learning</option>
                </select>
            </div>

            <div class="filter-group">
                <label for="searchBox">Search Algorithms:</label>
                <input type="text" id="searchBox" placeholder="Search by name or use case..." aria-label="Search algorithms">
            </div>

            <div class="filter-group">
                <label for="complexityFilter">
                    Filter by Complexity:
                    <span class="info-icon" 
                          onclick="showComplexityExplainer()" 
                          onkeypress="handleComplexityKeyPress(event)"
                          tabindex="0"
                          role="button"
                          aria-label="Learn about algorithm complexity">?</span>
                </label>
                <select id="complexityFilter" aria-label="Filter by complexity">
                    <option value="all">All Complexity Levels</option>
                    <option value="low">Low Complexity</option>
                    <option value="medium">Medium Complexity</option>
                    <option value="high">High Complexity</option>
                </select>
            </div>
        </div>

        <div class="stats-bar">
            <span>üìä Total Algorithms: <strong id="totalCount">0</strong></span>
            <span>üëÅÔ∏è Showing: <strong id="visibleCount">0</strong></span>
            <button class="helper-btn" onclick="showSelectionGuide()" 
                    onkeypress="handleHelperKeyPress(event, 'selection')"
                    aria-label="How to choose an algorithm">
                üéØ How to Choose
            </button>
            <button class="helper-btn" onclick="showMetricsGuide()" 
                    onkeypress="handleHelperKeyPress(event, 'metrics')"
                    aria-label="Evaluation metrics guide">
                üìà Metrics Guide
            </button>
        </div>

        <div class="algorithm-grid" id="algorithmGrid"></div>
    </div>

    <div class="overlay" id="overlay" onclick="closeExplainer(); closeComplexityExplainer(); closeSelectionGuide(); closeMetricsGuide();"></div>

    <div class="explainer-popup" id="explainerPopup" role="dialog" aria-modal="true" aria-labelledby="explainerTitle">
        <div class="explainer-header">
            <h3 id="explainerTitle">üéì Machine Learning Types Explained</h3>
            <button class="close-explainer" aria-label="Close explainer" onclick="closeExplainer()">&times;</button>
        </div>
        <div class="explainer-content">
            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üéØ</span>
                    Supervised Learning
                </div>
                <div class="learning-type-description">
                    Supervised learning uses <strong>labeled training data</strong> where both input features and correct outputs are provided. The algorithm learns to map inputs to outputs by finding patterns in the labeled examples. Think of it like learning with a teacher who provides the right answers.
                </div>
                <div class="learning-type-examples">
                    <strong>Examples:</strong> Linear Regression, Logistic Regression, Decision Trees, Random Forest, SVM, Neural Networks, KNN, Naive Bayes, Gradient Boosting, AdaBoost, ElasticNet
                    <br><br>
                    <strong>Common Tasks:</strong> Classification (spam detection, image recognition), Regression (price prediction, risk assessment)
                    <br><br>
                    <strong>Key Requirement:</strong> Need labeled data (input-output pairs)
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üîç</span>
                    Unsupervised Learning
                </div>
                <div class="learning-type-description">
                    Unsupervised learning works with <strong>unlabeled data</strong> where only input features are provided without corresponding outputs. The algorithm discovers hidden patterns, structures, or relationships in the data on its own. Like exploring data without a teacher.
                </div>
                <div class="learning-type-examples">
                    <strong>Examples:</strong> K-Means, Hierarchical Clustering, DBSCAN, Autoencoders, Gaussian Mixture Models (GMM), Isolation Forest
                    <br><br>
                    <strong>Common Tasks:</strong> Clustering (customer segmentation, pattern discovery), Anomaly Detection (fraud detection, outlier identification), Data Exploration
                    <br><br>
                    <strong>Key Requirement:</strong> No labels needed, finds structure in data
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üìâ</span>
                    Dimensionality Reduction
                </div>
                <div class="learning-type-description">
                    Dimensionality reduction techniques <strong>reduce the number of features</strong> in a dataset while preserving important information. They help with visualization, noise reduction, and speeding up other algorithms by removing redundant or less important features.
                </div>
                <div class="learning-type-examples">
                    <strong>Examples:</strong> PCA (Principal Component Analysis), t-SNE (t-Distributed Stochastic Neighbor Embedding)
                    <br><br>
                    <strong>Common Tasks:</strong> Data Visualization (2D/3D plots of high-dimensional data), Feature Engineering, Noise Reduction, Preprocessing for other ML models
                    <br><br>
                    <strong>Key Benefit:</strong> Makes data easier to understand and process
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üîÑ</span>
                    Self-supervised Learning
                </div>
                <div class="learning-type-description">
                    Self-supervised learning creates its own <strong>labels from the data itself</strong>. It uses part of the input to predict another part, learning useful representations without manual labeling. This approach has powered breakthroughs in NLP and computer vision.
                </div>
                <div class="learning-type-examples">
                    <strong>Examples:</strong> Transformers (BERT, GPT), Autoencoders (when used for representation learning)
                    <br><br>
                    <strong>Common Tasks:</strong> Language Understanding (predict masked words), Next Token Prediction, Image Representation Learning, Pre-training for Transfer Learning
                    <br><br>
                    <strong>Key Advantage:</strong> Learns from massive unlabeled datasets, then fine-tunes on smaller labeled data
                </div>
            </div>

            <div style="background: #e7f3ff; padding: 15px; border-radius: 8px; margin-top: 20px;">
                <strong>üí° Quick Decision Guide:</strong>
                <ul style="margin: 10px 0 0 20px; line-height: 1.8;">
                    <li><strong>Have labels?</strong> ‚Üí Use Supervised Learning</li>
                    <li><strong>No labels?</strong> ‚Üí Use Unsupervised Learning</li>
                    <li><strong>Too many features?</strong> ‚Üí Use Dimensionality Reduction first</li>
                    <li><strong>Massive unlabeled data?</strong> ‚Üí Consider Self-supervised Learning</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="explainer-popup" id="complexityPopup" role="dialog" aria-modal="true" aria-labelledby="complexityTitle">
        <div class="explainer-header">
            <h3 id="complexityTitle">‚öôÔ∏è Algorithm Complexity Explained</h3>
            <button class="close-explainer" aria-label="Close complexity explainer" onclick="closeComplexityExplainer()">&times;</button>
        </div>
        <div class="explainer-content">
            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üü¢</span>
                    Low Complexity
                </div>
                <div class="learning-type-description">
                    Low complexity algorithms are <strong>easy to understand, implement, and interpret</strong>. They typically have few hyperparameters, train quickly, and their decision-making process is transparent. Perfect for beginners and when interpretability is crucial.
                </div>
                <div class="learning-type-examples">
                    <strong>Characteristics:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li>Simple mathematical foundations</li>
                        <li>Fast training and prediction times</li>
                        <li>Easy to explain to non-technical stakeholders</li>
                        <li>Few parameters to tune</li>
                        <li>Lower computational requirements</li>
                    </ul>
                    <strong>Examples:</strong> Linear Regression, Logistic Regression, KNN, Naive Bayes, Decision Tree, K-Means, ElasticNet, Isolation Forest
                    <br><br>
                    <strong>Best For:</strong> Small datasets, baseline models, when interpretability matters, quick prototyping, educational purposes
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üü°</span>
                    Medium Complexity
                </div>
                <div class="learning-type-description">
                    Medium complexity algorithms offer a <strong>balance between performance and interpretability</strong>. They may have more hyperparameters to tune and require moderate computational resources, but still provide good understanding of how they work.
                </div>
                <div class="learning-type-examples">
                    <strong>Characteristics:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li>Moderate mathematical complexity</li>
                        <li>Several hyperparameters to optimize</li>
                        <li>Reasonable training times on typical hardware</li>
                        <li>Good performance-interpretability trade-off</li>
                        <li>May require some feature engineering</li>
                    </ul>
                    <strong>Examples:</strong> Random Forest, SVM, Hierarchical Clustering, PCA, DBSCAN, Autoencoders, GMM, AdaBoost
                    <br><br>
                    <strong>Best For:</strong> Production systems, when you need better accuracy than simple models, moderate-sized datasets, when some interpretability is needed
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üî¥</span>
                    High Complexity
                </div>
                <div class="learning-type-description">
                    High complexity algorithms are <strong>powerful but opaque</strong>. They can capture intricate patterns and achieve state-of-the-art performance but require significant expertise, computational resources, and large datasets. Often considered "black boxes."
                </div>
                <div class="learning-type-examples">
                    <strong>Characteristics:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li>Complex mathematical foundations (calculus, linear algebra)</li>
                        <li>Many hyperparameters requiring expertise to tune</li>
                        <li>Long training times, often needs GPUs</li>
                        <li>Difficult to interpret (black box models)</li>
                        <li>Requires large amounts of data</li>
                        <li>Prone to overfitting without careful tuning</li>
                    </ul>
                    <strong>Examples:</strong> Neural Networks (MLP), CNN, RNN, Transformers (BERT, GPT), Gradient Boosting, t-SNE
                    <br><br>
                    <strong>Best For:</strong> Large datasets, when maximum accuracy is critical, complex patterns (images, text, speech), when interpretability is secondary
                </div>
            </div>

            <div style="background: #fff3cd; padding: 15px; border-radius: 8px; margin-top: 20px; border-left: 4px solid #ffc107;">
                <strong>‚ö° Practical Tip:</strong> Start with low complexity algorithms to establish baselines. Only move to higher complexity if:
                <ul style="margin: 10px 0 0 20px; line-height: 1.8;">
                    <li>Simple models don't achieve required performance</li>
                    <li>You have sufficient data (rule of thumb: 10x samples per parameter)</li>
                    <li>You have computational resources and time</li>
                    <li>You have expertise to tune and validate the model</li>
                    <li>Interpretability is not a critical requirement</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="explainer-popup" id="selectionPopup" role="dialog" aria-modal="true" aria-labelledby="selectionTitle">
        <div class="explainer-header">
            <h3 id="selectionTitle">üéØ How to Choose the Right Algorithm</h3>
            <button class="close-explainer" aria-label="Close selection guide" onclick="closeSelectionGuide()">&times;</button>
        </div>
        <div class="explainer-content">
            <div style="background: #e7f3ff; padding: 15px; border-radius: 8px; margin-bottom: 20px; border-left: 4px solid #0066cc;">
                <strong>üß≠ The 5-Step Selection Process</strong>
                <p style="margin: 10px 0 0 0; color: #555;">Follow these steps to identify the best algorithm for your problem:</p>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">1Ô∏è‚É£</span>
                    Define Your Problem Type
                </div>
                <div class="learning-type-description">
                    Start by identifying what you're trying to accomplish:
                </div>
                <div class="learning-type-examples">
                    <strong>üìä Prediction Problems (Supervised):</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>Classification:</strong> Predict categories (spam/not spam, cat/dog/bird)
                            <br>‚Üí Use: Logistic Regression, SVM, Random Forest, Neural Networks</li>
                        <li><strong>Binary Classification:</strong> Two outcomes (yes/no, fraud/legitimate)
                            <br>‚Üí Use: Logistic Regression, SVM, Decision Tree, AdaBoost</li>
                        <li><strong>Multi-class Classification:</strong> 3+ categories
                            <br>‚Üí Use: Random Forest, Gradient Boosting, Neural Networks</li>
                        <li><strong>Regression:</strong> Predict continuous numbers (price, temperature, age)
                            <br>‚Üí Use: Linear Regression, ElasticNet, Random Forest, Gradient Boosting</li>
                    </ul>
                    
                    <strong>üîç Pattern Discovery (Unsupervised):</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>Clustering:</strong> Group similar items
                            <br>‚Üí Use: K-Means (spherical clusters), DBSCAN (arbitrary shapes), GMM (probabilistic)</li>
                        <li><strong>Anomaly Detection:</strong> Find outliers
                            <br>‚Üí Use: Isolation Forest, Autoencoders, DBSCAN</li>
                        <li><strong>Dimensionality Reduction:</strong> Reduce features
                            <br>‚Üí Use: PCA (linear), t-SNE (visualization)</li>
                    </ul>
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">2Ô∏è‚É£</span>
                    Consider Your Data
                </div>
                <div class="learning-type-examples">
                    <strong>Data Size:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>Small (&lt;1K samples):</strong> Naive Bayes, Linear/Logistic Regression, KNN</li>
                        <li><strong>Medium (1K-100K):</strong> SVM, Random Forest, Gradient Boosting</li>
                        <li><strong>Large (100K+):</strong> Neural Networks, Deep Learning, SGD-based methods</li>
                    </ul>
                    
                    <strong>Data Type:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>Images:</strong> CNN, Transfer Learning (ResNet, VGG)</li>
                        <li><strong>Text:</strong> Naive Bayes, Transformers (BERT, GPT), RNN</li>
                        <li><strong>Time Series:</strong> RNN, LSTM, ARIMA</li>
                        <li><strong>Tabular:</strong> Random Forest, Gradient Boosting, Neural Networks</li>
                        <li><strong>Spatial/Geographic:</strong> DBSCAN, GMM, CNN</li>
                    </ul>
                    
                    <strong>Data Quality:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>Clean data:</strong> Any algorithm appropriate for task</li>
                        <li><strong>Noisy/outliers:</strong> Random Forest, Gradient Boosting (robust), avoid SVM</li>
                        <li><strong>Missing values:</strong> Decision Trees, Random Forest, KNN (with imputation)</li>
                        <li><strong>Highly correlated features:</strong> ElasticNet, PCA preprocessing</li>
                    </ul>
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">3Ô∏è‚É£</span>
                    Evaluate Requirements
                </div>
                <div class="learning-type-examples">
                    <strong>Speed Requirements:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>Real-time predictions:</strong> Linear/Logistic Regression, Naive Bayes, KNN</li>
                        <li><strong>Batch processing OK:</strong> Random Forest, Gradient Boosting, Neural Networks</li>
                        <li><strong>Training speed matters:</strong> Linear models, Naive Bayes</li>
                    </ul>
                    
                    <strong>Interpretability:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>High (explain decisions):</strong> Linear/Logistic Regression, Decision Tree</li>
                        <li><strong>Medium:</strong> Random Forest (feature importance), Naive Bayes</li>
                        <li><strong>Low (black box OK):</strong> Neural Networks, Gradient Boosting, SVM</li>
                    </ul>
                    
                    <strong>Accuracy vs Simplicity:</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>Need maximum accuracy:</strong> Gradient Boosting, Neural Networks, Ensemble methods</li>
                        <li><strong>Baseline/prototype:</strong> Linear Regression, Logistic Regression, Decision Tree</li>
                        <li><strong>Production-ready balance:</strong> Random Forest, SVM, AdaBoost</li>
                    </ul>
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">4Ô∏è‚É£</span>
                    Quick Decision Chart
                </div>
                <div class="learning-type-examples" style="background: #f8f9fa;">
                    <table style="width: 100%; border-collapse: collapse; font-size: 0.9em;">
                        <tr style="background: #1e3c72; color: white;">
                            <th style="padding: 10px; text-align: left;">If you have...</th>
                            <th style="padding: 10px; text-align: left;">Start with...</th>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 10px;">Labeled continuous target</td>
                            <td style="padding: 10px;"><strong>Linear Regression</strong> ‚Üí ElasticNet ‚Üí Random Forest</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd; background: #f8f9fa;">
                            <td style="padding: 10px;">Labeled binary target</td>
                            <td style="padding: 10px;"><strong>Logistic Regression</strong> ‚Üí SVM ‚Üí Random Forest</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 10px;">Labeled multi-class target</td>
                            <td style="padding: 10px;"><strong>Random Forest</strong> ‚Üí Gradient Boosting ‚Üí Neural Networks</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd; background: #f8f9fa;">
                            <td style="padding: 10px;">Unlabeled data (need groups)</td>
                            <td style="padding: 10px;"><strong>K-Means</strong> ‚Üí DBSCAN ‚Üí GMM</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 10px;">Unlabeled (find outliers)</td>
                            <td style="padding: 10px;"><strong>Isolation Forest</strong> ‚Üí DBSCAN ‚Üí Autoencoder</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd; background: #f8f9fa;">
                            <td style="padding: 10px;">Images</td>
                            <td style="padding: 10px;"><strong>CNN</strong> ‚Üí Transfer Learning (ResNet)</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 10px;">Text/sequences</td>
                            <td style="padding: 10px;"><strong>Naive Bayes</strong> ‚Üí RNN/LSTM ‚Üí Transformers</td>
                        </tr>
                        <tr style="background: #f8f9fa;">
                            <td style="padding: 10px;">Too many features</td>
                            <td style="padding: 10px;"><strong>PCA</strong> ‚Üí then your chosen algorithm</td>
                        </tr>
                    </table>
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">5Ô∏è‚É£</span>
                    The Golden Rule
                </div>
                <div class="learning-type-description" style="background: #d4edda; padding: 15px; border-radius: 8px; border-left: 4px solid #28a745;">
                    <strong>Always start simple, then increase complexity!</strong>
                    <br><br>
                    1. Begin with a simple baseline (Linear/Logistic Regression, Decision Tree)
                    <br>2. Evaluate performance using appropriate metrics
                    <br>3. Only increase complexity if needed (Random Forest, Gradient Boosting)
                    <br>4. Try ensemble methods or neural networks if still not sufficient
                    <br>5. Remember: A simple model that works is better than a complex model you can't debug
                </div>
            </div>

            <div style="background: #fff3cd; padding: 15px; border-radius: 8px; margin-top: 20px; border-left: 4px solid #ffc107;">
                <strong>üí° Pro Tips:</strong>
                <ul style="margin: 10px 0 0 20px; line-height: 1.8;">
                    <li>Try multiple algorithms and compare (use cross-validation)</li>
                    <li>Ensemble different models for better results</li>
                    <li>Domain knowledge beats fancy algorithms</li>
                    <li>More data usually helps more than better algorithms</li>
                    <li>Feature engineering is often more important than algorithm choice</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="explainer-popup" id="metricsPopup" role="dialog" aria-modal="true" aria-labelledby="metricsTitle">
        <div class="explainer-header">
            <h3 id="metricsTitle">üìà Evaluation Metrics Guide</h3>
            <button class="close-explainer" aria-label="Close metrics guide" onclick="closeMetricsGuide()">&times;</button>
        </div>
        <div class="explainer-content">
            <div style="background: #e7f3ff; padding: 15px; border-radius: 8px; margin-bottom: 20px; border-left: 4px solid #0066cc;">
                <strong>üìä Why Metrics Matter</strong>
                <p style="margin: 10px 0 0 0; color: #555;">Different problems require different metrics. Choosing the wrong metric can make a bad model look good! Learn which metrics to use for your specific task.</p>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üìä</span>
                    Classification Metrics
                </div>
                <div class="learning-type-description">
                    For predicting categories (spam/not spam, disease/healthy, cat/dog):
                </div>
                <div class="learning-type-examples">
                    <strong>üéØ Accuracy</strong>
                    <br><em>Formula: (Correct Predictions) / (Total Predictions)</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Overall correctness percentage</li>
                        <li><strong>When to use:</strong> Balanced classes (similar number in each category)</li>
                        <li><strong>When NOT to use:</strong> Imbalanced data (99% accuracy on 99% negative class is meaningless)</li>
                        <li><strong>Example:</strong> 95% accuracy = model is correct 95 out of 100 times</li>
                    </ul>

                    <strong>üéØ Precision</strong>
                    <br><em>Formula: True Positives / (True Positives + False Positives)</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Of all positive predictions, how many were actually correct?</li>
                        <li><strong>When to use:</strong> When false positives are costly (spam filter marking important email as spam)</li>
                        <li><strong>Use cases:</strong> Document classification, medical diagnosis (false alarms expensive)</li>
                        <li><strong>Example:</strong> 90% precision = 9 out of 10 "spam" predictions are actually spam</li>
                    </ul>

                    <strong>üéØ Recall (Sensitivity)</strong>
                    <br><em>Formula: True Positives / (True Positives + False Negatives)</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Of all actual positives, how many did we find?</li>
                        <li><strong>When to use:</strong> When false negatives are costly (missing cancer diagnosis)</li>
                        <li><strong>Use cases:</strong> Fraud detection, disease screening, security threats</li>
                        <li><strong>Example:</strong> 85% recall = we catch 85 out of 100 actual fraud cases</li>
                    </ul>

                    <strong>üéØ F1-Score</strong>
                    <br><em>Formula: 2 √ó (Precision √ó Recall) / (Precision + Recall)</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Harmonic mean of precision and recall</li>
                        <li><strong>When to use:</strong> Imbalanced classes, need balance between precision and recall</li>
                        <li><strong>Use cases:</strong> Most classification tasks, especially imbalanced data</li>
                        <li><strong>Example:</strong> F1 = 0.88 means good balance (scale: 0 to 1)</li>
                    </ul>

                    <strong>üéØ ROC-AUC (Area Under ROC Curve)</strong>
                    <br><em>Plots True Positive Rate vs False Positive Rate</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Model's ability to distinguish between classes</li>
                        <li><strong>When to use:</strong> Binary classification, comparing models</li>
                        <li><strong>Interpretation:</strong> 0.5 = random, 1.0 = perfect, 0.7-0.8 = acceptable, 0.8-0.9 = excellent</li>
                        <li><strong>Use cases:</strong> Medical diagnosis, fraud detection, any binary classification</li>
                    </ul>

                    <strong>üéØ Confusion Matrix</strong>
                    <br><em>Table showing True Positives, False Positives, True Negatives, False Negatives</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it shows:</strong> Detailed breakdown of correct and incorrect predictions</li>
                        <li><strong>When to use:</strong> Understanding where your model is making mistakes</li>
                        <li><strong>Best for:</strong> Debugging classification models</li>
                    </ul>
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üìâ</span>
                    Regression Metrics
                </div>
                <div class="learning-type-description">
                    For predicting continuous values (prices, temperatures, ages):
                </div>
                <div class="learning-type-examples">
                    <strong>üìê Mean Absolute Error (MAE)</strong>
                    <br><em>Formula: Average of |Predicted - Actual|</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Average magnitude of errors (in same units as target)</li>
                        <li><strong>When to use:</strong> Easy interpretation, all errors equally important</li>
                        <li><strong>Example:</strong> MAE = $5,000 means predictions off by $5,000 on average</li>
                        <li><strong>Advantage:</strong> Not sensitive to outliers</li>
                    </ul>

                    <strong>üìê Mean Squared Error (MSE)</strong>
                    <br><em>Formula: Average of (Predicted - Actual)¬≤</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Average squared errors (penalizes large errors more)</li>
                        <li><strong>When to use:</strong> Large errors are particularly bad</li>
                        <li><strong>Units:</strong> Squared units (harder to interpret)</li>
                        <li><strong>Disadvantage:</strong> Sensitive to outliers</li>
                    </ul>

                    <strong>üìê Root Mean Squared Error (RMSE)</strong>
                    <br><em>Formula: Square root of MSE</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Same as MSE but in original units</li>
                        <li><strong>When to use:</strong> Want MSE properties but interpretable units</li>
                        <li><strong>Example:</strong> RMSE = $7,500 for house prices</li>
                        <li><strong>Most common:</strong> Default metric for many regression problems</li>
                    </ul>

                    <strong>üìê R-squared (R¬≤)</strong>
                    <br><em>Formula: 1 - (Sum of Squared Errors / Total Variance)</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Proportion of variance explained by model (0 to 1)</li>
                        <li><strong>Interpretation:</strong> 0 = model is useless, 1 = perfect predictions</li>
                        <li><strong>Example:</strong> R¬≤ = 0.85 means model explains 85% of variance</li>
                        <li><strong>Common values:</strong> 0.7-0.9 is typically good</li>
                    </ul>

                    <strong>üìê Mean Absolute Percentage Error (MAPE)</strong>
                    <br><em>Formula: Average of |((Predicted - Actual) / Actual)| √ó 100%</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Average percentage error</li>
                        <li><strong>When to use:</strong> Want scale-independent metric, easy business interpretation</li>
                        <li><strong>Example:</strong> MAPE = 15% means predictions off by 15% on average</li>
                        <li><strong>Warning:</strong> Undefined when actual values are zero</li>
                    </ul>
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">üîç</span>
                    Clustering Metrics
                </div>
                <div class="learning-type-description">
                    For evaluating unsupervised grouping (when you don't have true labels):
                </div>
                <div class="learning-type-examples">
                    <strong>üé® Silhouette Score</strong>
                    <br><em>Range: -1 to +1</em>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> How similar points are to their own cluster vs other clusters</li>
                        <li><strong>Interpretation:</strong> +1 = perfect, 0 = overlapping, -1 = wrong clusters</li>
                        <li><strong>Use:</strong> Determining optimal number of clusters</li>
                        <li><strong>Best for:</strong> K-Means, GMM validation</li>
                    </ul>

                    <strong>üé® Davies-Bouldin Index</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Average similarity between clusters (lower is better)</li>
                        <li><strong>Interpretation:</strong> Lower values = better separation</li>
                        <li><strong>Use:</strong> Comparing different clustering solutions</li>
                    </ul>

                    <strong>üé® Inertia (Within-Cluster Sum of Squares)</strong>
                    <ul style="margin: 10px 0 10px 20px; line-height: 1.8;">
                        <li><strong>What it measures:</strong> Sum of squared distances to nearest cluster center</li>
                        <li><strong>Use:</strong> Elbow method to find optimal K</li>
                        <li><strong>Best for:</strong> K-Means optimization</li>
                    </ul>
                </div>
            </div>

            <div class="learning-type">
                <div class="learning-type-title">
                    <span class="learning-type-icon">‚öñÔ∏è</span>
                    Quick Selection Guide
                </div>
                <div class="learning-type-examples" style="background: #f8f9fa;">
                    <table style="width: 100%; border-collapse: collapse; font-size: 0.9em;">
                        <tr style="background: #1e3c72; color: white;">
                            <th style="padding: 10px; text-align: left;">Your Situation</th>
                            <th style="padding: 10px; text-align: left;">Use This Metric</th>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 10px;">Balanced classification, care equally about all errors</td>
                            <td style="padding: 10px;"><strong>Accuracy</strong></td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd; background: #f8f9fa;">
                            <td style="padding: 10px;">Imbalanced classification</td>
                            <td style="padding: 10px;"><strong>F1-Score, ROC-AUC</strong></td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 10px;">False positives are very costly</td>
                            <td style="padding: 10px;"><strong>Precision</strong></td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd; background: #f8f9fa;">
                            <td style="padding: 10px;">False negatives are very costly</td>
                            <td style="padding: 10px;"><strong>Recall</strong></td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 10px;">Regression, easy interpretation</td>
                            <td style="padding: 10px;"><strong>MAE, RMSE</strong></td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd; background: #f8f9fa;">
                            <td style="padding: 10px;">Regression, percentage error</td>
                            <td style="padding: 10px;"><strong>MAPE</strong></td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 10px;">Regression, explain variance</td>
                            <td style="padding: 10px;"><strong>R-squared</strong></td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd; background: #f8f9fa;">
                            <td style="padding: 10px;">Clustering quality</td>
                            <td style="padding: 10px;"><strong>Silhouette Score</strong></td>
                        </tr>
                        <tr style="background: #f8f9fa;">
                            <td style="padding: 10px;">Finding optimal K for K-Means</td>
                            <td style="padding: 10px;"><strong>Elbow Method (Inertia)</strong></td>
                        </tr>
                    </table>
                </div>
            </div>

            <div style="background: #fff3cd; padding: 15px; border-radius: 8px; margin-top: 20px; border-left: 4px solid #ffc107;">
                <strong>‚ö†Ô∏è Common Mistakes to Avoid:</strong>
                <ul style="margin: 10px 0 0 20px; line-height: 1.8;">
                    <li>Using accuracy on imbalanced data (99% accuracy can be useless!)</li>
                    <li>Optimizing for the wrong metric (precision when you need recall)</li>
                    <li>Not using cross-validation (single train-test split can be misleading)</li>
                    <li>Comparing RMSE across different scales (always check units)</li>
                    <li>Ignoring business context (5% error might be great or terrible)</li>
                </ul>
            </div>

            <div style="background: #d4edda; padding: 15px; border-radius: 8px; margin-top: 20px; border-left: 4px solid #28a745;">
                <strong>‚úÖ Best Practices:</strong>
                <ul style="margin: 10px 0 0 20px; line-height: 1.8;">
                    <li>Always use multiple metrics to get full picture</li>
                    <li>Plot confusion matrix for classification problems</li>
                    <li>Use cross-validation for reliable estimates</li>
                    <li>Consider business impact, not just statistical metrics</li>
                    <li>Compare against simple baseline (majority class, mean prediction)</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="modal" id="detailModal" role="dialog" aria-modal="true" aria-labelledby="modalTitle">
        <div class="modal-content">
            <div class="modal-header">
                <h2 id="modalTitle"></h2>
                <button class="close-modal" aria-label="Close modal" onclick="closeModal()">&times;</button>
            </div>
            <div class="modal-body" id="modalBody"></div>
        </div>
    </div>

    <footer>
        <p>Machine Learning Algorithms Reference Guide | Educational Tool</p>
        <p style="font-size: 0.9em; margin-top: 10px;">Navigate with keyboard: Tab, Enter, Escape</p>
    </footer>

    <script>
        const algorithms = [
            {
                name: "Linear Regression",
                type: "Supervised",
                bestUse: "Predicting continuous values",
                formula: "Y = b0 + b1X + b2X2 + ...",
                assumptions: "Linearity, independence",
                pros: "Simple, interpretable, fast",
                cons: "Sensitive to outliers, non-linear limits",
                whenNotToUse: "Data with strong non-linearity",
                realWorldExample: "House price prediction",
                complexity: "low",
                detailedDescription: "Linear Regression is one of the most fundamental algorithms in machine learning. It models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The algorithm finds the best-fitting straight line through the data points by minimizing the sum of squared residuals.",
                additionalUseCases: [
                    "Sales forecasting based on advertising spend",
                    "Risk assessment in insurance",
                    "Economic trend analysis",
                    "Temperature prediction based on historical data",
                    "Stock market trend analysis (simple models)",
                    "Agricultural yield prediction"
                ],
                technicalDetails: "Uses Ordinary Least Squares (OLS) method to minimize prediction errors. Assumes homoscedasticity (constant variance of errors) and normality of residuals. Performance measured using R-squared, Mean Squared Error (MSE), or Root Mean Squared Error (RMSE).",
                preprocessing: "Requires feature scaling/normalization. Check for multicollinearity among features. Handle missing values appropriately.",
                variants: "Multiple Linear Regression, Polynomial Regression, Ridge Regression, Lasso Regression, Elastic Net"
            },
            {
                name: "Logistic Regression",
                type: "Supervised",
                bestUse: "Binary classification",
                formula: "P = 1 / (1 + e^-(b0 + b1X + ...))",
                assumptions: "Log-odds linearity",
                pros: "Probabilistic, interpretable",
                cons: "Weak with non-linear boundaries",
                whenNotToUse: "Data is highly non-linear",
                realWorldExample: "Spam detection",
                complexity: "low",
                detailedDescription: "Logistic Regression is a statistical method for binary classification that predicts the probability of an observation belonging to a particular class. Despite its name, it's used for classification, not regression. It uses a sigmoid function to map predictions to probabilities between 0 and 1.",
                additionalUseCases: [
                    "Email spam filtering",
                    "Credit card fraud detection",
                    "Disease diagnosis (presence/absence)",
                    "Customer churn prediction",
                    "Marketing campaign response prediction",
                    "Loan default prediction",
                    "Employee turnover prediction"
                ],
                technicalDetails: "Uses maximum likelihood estimation to find optimal parameters. Decision boundary is linear in feature space. Can be extended to multi-class classification using one-vs-rest or softmax approaches. Regularization (L1/L2) helps prevent overfitting.",
                preprocessing: "Feature scaling recommended. Handle class imbalance. Create polynomial features for non-linear boundaries.",
                variants: "Multinomial Logistic Regression, Ordinal Logistic Regression, Regularized Logistic Regression"
            },
            {
                name: "Decision Tree",
                type: "Supervised",
                bestUse: "Classification / Regression",
                formula: "Recursive binary split",
                assumptions: "None",
                pros: "Easy to interpret",
                cons: "Overfitting, unstable",
                whenNotToUse: "Noisy or complex datasets",
                realWorldExample: "Loan default prediction",
                complexity: "low",
                detailedDescription: "Decision Trees create a flowchart-like structure where each internal node represents a test on an attribute, each branch represents the outcome, and each leaf node represents a class label or value. Trees recursively split data based on feature values that maximize information gain or minimize impurity.",
                additionalUseCases: [
                    "Medical diagnosis systems",
                    "Customer segmentation",
                    "Risk assessment",
                    "Fraud detection",
                    "Product recommendation",
                    "Credit scoring",
                    "Employee performance evaluation"
                ],
                technicalDetails: "Uses metrics like Gini impurity, entropy, or information gain for splitting. Can handle both numerical and categorical data. Prone to overfitting without pruning. Maximum depth and minimum samples per leaf control complexity.",
                preprocessing: "Minimal preprocessing required. Can handle missing values. No need for feature scaling.",
                variants: "CART (Classification and Regression Trees), ID3, C4.5, C5.0"
            },
            {
                name: "Random Forest",
                type: "Supervised",
                bestUse: "Ensemble accuracy",
                formula: "Bagging + averaging trees",
                assumptions: "Tree independence",
                pros: "High accuracy, robust",
                cons: "Slower, less interpretable",
                whenNotToUse: "Need real-time results",
                realWorldExample: "Fraud detection",
                complexity: "medium",
                detailedDescription: "Random Forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of classes (classification) or mean prediction (regression) of individual trees. It uses bagging and feature randomness to create an uncorrelated forest of trees whose prediction is more accurate than any individual tree.",
                additionalUseCases: [
                    "Credit risk assessment",
                    "Healthcare outcome prediction",
                    "Feature importance ranking",
                    "Anomaly detection",
                    "Remote sensing land use classification",
                    "Bioinformatics gene expression analysis",
                    "Market trend prediction",
                    "Quality control in manufacturing"
                ],
                technicalDetails: "Combines multiple decision trees using bootstrap sampling and random feature selection. Reduces variance without increasing bias. Out-of-bag (OOB) error provides internal validation. Feature importance scores help with interpretation.",
                preprocessing: "Handles mixed data types well. Robust to outliers. Can handle missing values through proximity measures.",
                variants: "Extremely Randomized Trees (Extra Trees), Isolation Forest (for anomaly detection)"
            },
            {
                name: "Gradient Boosting",
                type: "Supervised",
                bestUse: "High-performance modeling",
                formula: "Additive trees minimizing loss",
                assumptions: "Sequential dependency",
                pros: "State-of-the-art accuracy",
                cons: "Overfitting, needs tuning",
                whenNotToUse: "When interpretability matters",
                realWorldExample: "Credit scoring",
                complexity: "high",
                detailedDescription: "Gradient Boosting builds an ensemble of weak learners (typically decision trees) sequentially, where each new tree corrects errors made by previously trained trees. It optimizes a differentiable loss function using gradient descent in function space, making it one of the most powerful machine learning algorithms.",
                additionalUseCases: [
                    "Ranking problems (search engines)",
                    "Click-through rate prediction",
                    "Customer lifetime value prediction",
                    "Risk modeling in finance",
                    "Demand forecasting",
                    "Kaggle competitions (frequently wins)",
                    "Predictive maintenance",
                    "Insurance claim prediction"
                ],
                technicalDetails: "Uses gradient descent to minimize loss function. Learning rate controls contribution of each tree. Supports various loss functions for different tasks. XGBoost, LightGBM, and CatBoost are popular implementations with optimizations.",
                preprocessing: "Requires careful hyperparameter tuning. Feature engineering important. Handle missing values explicitly.",
                variants: "XGBoost, LightGBM, CatBoost, AdaBoost, GradientBoost Regressor/Classifier"
            },
            {
                name: "Support Vector Machine (SVM)",
                type: "Supervised",
                bestUse: "Max-margin classification",
                formula: "Maximize margin using kernel trick",
                assumptions: "Separability, scaling",
                pros: "Works in high dimensions",
                cons: "Slow on large data",
                whenNotToUse: "Large noisy datasets",
                realWorldExample: "Facial recognition",
                complexity: "medium",
                detailedDescription: "Support Vector Machines find the optimal hyperplane that maximizes the margin between different classes in high-dimensional space. SVMs can use kernel functions to transform data into higher dimensions where linear separation becomes possible, making them effective for complex non-linear problems.",
                additionalUseCases: [
                    "Text classification and sentiment analysis",
                    "Image classification",
                    "Handwriting recognition",
                    "Bioinformatics (protein classification)",
                    "Cancer classification from gene expression",
                    "Face detection",
                    "Intrusion detection systems"
                ],
                technicalDetails: "Uses kernel trick (linear, polynomial, RBF, sigmoid) to handle non-linear boundaries. Regularization parameter C controls trade-off between margin maximization and classification error. Support vectors are critical data points that define the decision boundary.",
                preprocessing: "Feature scaling critical. Remove outliers. Consider kernel selection carefully.",
                variants: "Linear SVM, Kernel SVM (RBF, Polynomial), Nu-SVM, One-Class SVM"
            },
            {
                name: "K-Nearest Neighbors (KNN)",
                type: "Supervised",
                bestUse: "Few-shot classification",
                formula: "Distance-based majority vote",
                assumptions: "Feature scaling",
                pros: "Simple, no training phase",
                cons: "Slow, noisy sensitive",
                whenNotToUse: "High-dimensional noisy data",
                realWorldExample: "Recommender systems",
                complexity: "low",
                detailedDescription: "K-Nearest Neighbors is a non-parametric, instance-based learning algorithm that classifies new data points based on similarity measures (distance) to K nearest training examples. It's lazy learning because it doesn't build an explicit model but stores all training data for prediction time.",
                additionalUseCases: [
                    "Recommendation engines (collaborative filtering)",
                    "Pattern recognition",
                    "Credit rating",
                    "Predicting loan defaults",
                    "Market segmentation",
                    "Missing value imputation",
                    "Anomaly detection",
                    "Gene expression analysis"
                ],
                technicalDetails: "Common distance metrics: Euclidean, Manhattan, Minkowski, Hamming. Choice of K is critical - too small leads to overfitting, too large oversimplifies. Weighted voting can improve performance. Computationally expensive at prediction time.",
                preprocessing: "Feature scaling essential. Remove irrelevant features (curse of dimensionality). Handle missing values.",
                variants: "Weighted KNN, Ball Tree KNN, KD-Tree KNN"
            },
            {
                name: "Naive Bayes",
                type: "Supervised",
                bestUse: "Text classification",
                formula: "Bayes theorem + feature independence",
                assumptions: "Independent features",
                pros: "Fast, good with text data",
                cons: "Fails with correlated features",
                whenNotToUse: "Feature dependency present",
                realWorldExample: "Sentiment analysis",
                complexity: "low",
                detailedDescription: "Naive Bayes is a probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between features. Despite this simplistic assumption, it performs surprisingly well in many real-world applications, especially text classification.",
                additionalUseCases: [
                    "Spam filtering (email classification)",
                    "Document categorization",
                    "Medical diagnosis",
                    "Weather prediction",
                    "Real-time prediction applications",
                    "Multi-class classification problems",
                    "Recommendation systems",
                    "Credit risk assessment"
                ],
                technicalDetails: "Variants include Gaussian NB (continuous features), Multinomial NB (discrete counts), and Bernoulli NB (binary features). Requires relatively small training data. Handles missing values well. Provides probability estimates for predictions.",
                preprocessing: "Works well with text data (TF-IDF, bag of words). Laplace smoothing handles zero probabilities.",
                variants: "Gaussian Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes"
            },
            {
                name: "K-Means Clustering",
                type: "Unsupervised",
                bestUse: "Customer segmentation",
                formula: "Minimize intra-cluster distance",
                assumptions: "Spherical, equal clusters",
                pros: "Fast, easy to implement",
                cons: "Needs K, sensitive to scale",
                whenNotToUse: "Non-spherical data",
                realWorldExample: "Customer segmentation",
                complexity: "low",
                detailedDescription: "K-Means is an unsupervised learning algorithm that partitions data into K distinct clusters by minimizing within-cluster variance. It iteratively assigns points to the nearest centroid and recalculates centroids until convergence, making it efficient for large datasets.",
                additionalUseCases: [
                    "Market segmentation",
                    "Image compression and segmentation",
                    "Document clustering",
                    "Crime locality detection",
                    "Delivery route optimization",
                    "Identifying accident-prone areas",
                    "Customer behavior analysis",
                    "Anomaly detection"
                ],
                technicalDetails: "Uses Euclidean distance by default. K must be specified beforehand (use elbow method or silhouette analysis). Sensitive to initial centroid placement (K-means++ improves initialization). Local optima possible - run multiple times with different initializations.",
                preprocessing: "Feature scaling critical. Handle outliers. Choose optimal K value.",
                variants: "K-means++, Mini-batch K-means, Fuzzy C-means"
            },
            {
                name: "Hierarchical Clustering",
                type: "Unsupervised",
                bestUse: "Data structure understanding",
                formula: "Nested dendrogram",
                assumptions: "Distance metric",
                pros: "No need for K, visual",
                cons: "Memory and compute intensive",
                whenNotToUse: "Very large datasets",
                realWorldExample: "Gene expression analysis",
                complexity: "medium",
                detailedDescription: "Hierarchical Clustering creates a tree-like structure (dendrogram) of clusters by either merging small clusters into larger ones (agglomerative) or dividing large clusters into smaller ones (divisive). It doesn't require specifying the number of clusters beforehand and provides insight into data structure at multiple scales.",
                additionalUseCases: [
                    "Taxonomy creation in biology",
                    "Social network analysis",
                    "Document organization",
                    "City planning and zoning",
                    "Image segmentation",
                    "Phylogenetic tree construction",
                    "Product categorization",
                    "Customer segmentation"
                ],
                technicalDetails: "Linkage methods include single, complete, average, and Ward's. Different linkage methods produce different cluster shapes. Can use various distance metrics (Euclidean, Manhattan, cosine). Cut dendrogram at desired level to get clusters.",
                preprocessing: "Feature scaling important. Choose appropriate distance metric and linkage method.",
                variants: "Agglomerative (bottom-up), Divisive (top-down), BIRCH, CURE"
            },
            {
                name: "Principal Component Analysis (PCA)",
                type: "Dimensionality Reduction",
                bestUse: "Reducing feature dimensionality",
                formula: "Eigenvectors of covariance matrix",
                assumptions: "Large variance important",
                pros: "Noise reduction, speed-up",
                cons: "Hard to interpret",
                whenNotToUse: "All features are important",
                realWorldExample: "Image compression",
                complexity: "medium",
                detailedDescription: "PCA is an unsupervised dimensionality reduction technique that transforms data into a new coordinate system where the greatest variance lies on the first axis (first principal component), the second greatest variance on the second axis, and so on. It's used to reduce feature count while retaining most information.",
                additionalUseCases: [
                    "Data visualization (reducing to 2D/3D)",
                    "Noise filtering",
                    "Face recognition (Eigenfaces)",
                    "Feature extraction",
                    "Portfolio optimization in finance",
                    "Genomic data analysis",
                    "Signal processing",
                    "Preprocessing for other ML algorithms"
                ],
                technicalDetails: "Finds orthogonal axes that maximize variance. Components are uncorrelated. Eigenvalues indicate variance explained by each component. Scree plot helps determine number of components to retain. Standardization typically required before PCA.",
                preprocessing: "Standardize features (zero mean, unit variance). Remove missing values.",
                variants: "Kernel PCA, Incremental PCA, Sparse PCA, Probabilistic PCA"
            },
            {
                name: "Neural Networks (MLP)",
                type: "Supervised",
                bestUse: "Complex pattern modeling",
                formula: "Weighted sums + activation functions",
                assumptions: "Enough data, scaling",
                pros: "Non-linear learning power",
                cons: "Needs large data & tuning",
                whenNotToUse: "Small data, low compute",
                realWorldExample: "Image classification",
                complexity: "high",
                detailedDescription: "Multi-Layer Perceptrons (MLPs) are feedforward artificial neural networks with multiple layers of neurons. Each neuron applies a weighted sum of inputs followed by an activation function. Through backpropagation, MLPs learn complex non-linear relationships between inputs and outputs.",
                additionalUseCases: [
                    "Speech recognition",
                    "Time series prediction",
                    "Credit risk evaluation",
                    "Customer churn prediction",
                    "Medical diagnosis",
                    "Fraud detection",
                    "Natural language processing",
                    "Game playing AI"
                ],
                technicalDetails: "Common activation functions: ReLU, sigmoid, tanh. Optimizers include SGD, Adam, RMSprop. Dropout and batch normalization help prevent overfitting. Learning rate scheduling improves convergence. Requires significant computational resources.",
                preprocessing: "Feature normalization critical. Data augmentation helpful. Batch processing.",
                variants: "Deep Neural Networks (DNN), Residual Networks (ResNet), DenseNet"
            },
            {
                name: "Convolutional Neural Networks (CNN)",
                type: "Supervised",
                bestUse: "Image/video/spatial data",
                formula: "Convolution + pooling layers",
                assumptions: "Grid-like spatial data",
                pros: "Excellent for images",
                cons: "High resource demand",
                whenNotToUse: "Sequence/text data",
                realWorldExample: "Self-driving vision",
                complexity: "high",
                detailedDescription: "CNNs are specialized neural networks designed for processing grid-structured data like images. They use convolutional layers that apply filters to detect features (edges, textures, patterns) and pooling layers that reduce spatial dimensions while retaining important information. CNNs have revolutionized computer vision.",
                additionalUseCases: [
                    "Medical image analysis (X-rays, MRIs)",
                    "Facial recognition and verification",
                    "Object detection and localization",
                    "Image segmentation",
                    "Video analysis and action recognition",
                    "Autonomous vehicles",
                    "Satellite imagery analysis",
                    "Quality inspection in manufacturing",
                    "Art style transfer"
                ],
                technicalDetails: "Architecture includes convolutional layers, pooling layers, and fully connected layers. Popular architectures: VGG, ResNet, Inception, EfficientNet. Transfer learning allows using pre-trained models. Data augmentation crucial for generalization.",
                preprocessing: "Image normalization, data augmentation (rotation, flipping, cropping), resize to fixed dimensions.",
                variants: "VGGNet, ResNet, Inception, MobileNet, EfficientNet, U-Net, YOLO, R-CNN"
            },
            {
                name: "Recurrent Neural Networks (RNN)",
                type: "Supervised",
                bestUse: "Sequence modeling",
                formula: "Feedback loops over time",
                assumptions: "Sequential structure",
                pros: "Time-series & text ready",
                cons: "Vanishing gradient",
                whenNotToUse: "Long sequences",
                realWorldExample: "Stock prediction",
                complexity: "high",
                detailedDescription: "RNNs are neural networks designed for sequential data, maintaining hidden states that capture information from previous time steps. They excel at tasks where context and order matter, though vanilla RNNs struggle with long-term dependencies due to vanishing gradients.",
                additionalUseCases: [
                    "Language modeling and text generation",
                    "Machine translation",
                    "Speech recognition",
                    "Video analysis",
                    "Music generation",
                    "Handwriting recognition",
                    "Time series forecasting",
                    "Sentiment analysis",
                    "DNA sequence analysis"
                ],
                technicalDetails: "LSTM and GRU variants address vanishing gradient problem through gating mechanisms. Bidirectional RNNs process sequences in both directions. Teacher forcing used during training. Attention mechanisms enhance performance on long sequences.",
                preprocessing: "Sequence padding/truncation, tokenization for text, normalization for time series.",
                variants: "LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit), Bidirectional RNN"
            },
            {
                name: "Transformers (BERT, GPT)",
                type: "Supervised/Self-supervised",
                bestUse: "NLP tasks, chat, translation",
                formula: "Attention mechanism + position encoding",
                assumptions: "Large training data",
                pros: "Long context, fast",
                cons: "Heavy compute, large model",
                whenNotToUse: "Small projects",
                realWorldExample: "ChatGPT, translation tools",
                complexity: "high",
                detailedDescription: "Transformers revolutionized NLP by replacing recurrence with self-attention mechanisms, allowing parallel processing and better handling of long-range dependencies. Models like BERT (bidirectional encoding) and GPT (autoregressive generation) have achieved state-of-the-art results across numerous tasks.",
                additionalUseCases: [
                    "Question answering systems",
                    "Text summarization",
                    "Named entity recognition",
                    "Semantic search",
                    "Code generation and completion",
                    "Document classification",
                    "Conversational AI",
                    "Content generation",
                    "Language understanding tasks"
                ],
                technicalDetails: "Self-attention computes relationships between all tokens in parallel. Position encoding preserves sequential information. Pre-training on massive datasets followed by fine-tuning. BERT uses masked language modeling, GPT uses next-token prediction. Requires significant computational resources (GPUs/TPUs).",
                preprocessing: "Tokenization (WordPiece, BPE), special tokens ([CLS], [SEP]), attention masks.",
                variants: "BERT, GPT (1-4), T5, RoBERTa, ALBERT, DistilBERT, ELECTRA, XLNet"
            },
            {
                name: "Autoencoders",
                type: "Unsupervised",
                bestUse: "Compression & anomaly detection",
                formula: "Encoder-decoder + reconstruction loss",
                assumptions: "Symmetric network",
                pros: "Effective denoising",
                cons: "Can overfit, black-box",
                whenNotToUse: "When no compression needed",
                realWorldExample: "Fraud detection",
                complexity: "medium",
                detailedDescription: "Autoencoders are neural networks that learn efficient data encodings in an unsupervised manner. They compress input to a lower-dimensional representation (encoding) and then reconstruct it (decoding). The reconstruction error reveals anomalies and the encoder provides useful features.",
                additionalUseCases: [
                    "Image denoising and restoration",
                    "Dimensionality reduction",
                    "Feature learning",
                    "Data generation (VAE)",
                    "Anomaly detection in cybersecurity",
                    "Recommendation systems",
                    "Drug discovery",
                    "Credit card fraud detection"
                ],
                technicalDetails: "Encoder compresses to bottleneck layer, decoder reconstructs. Variants include denoising autoencoders, sparse autoencoders, and variational autoencoders (VAE). Loss function typically reconstruction error (MSE for images). Bottleneck size controls compression ratio.",
                preprocessing: "Normalize inputs to [0,1] or [-1,1]. No labels needed.",
                variants: "Denoising Autoencoder, Sparse Autoencoder, Variational Autoencoder (VAE), Contractive Autoencoder"
            },
            {
                name: "DBSCAN",
                type: "Unsupervised",
                bestUse: "Arbitrary shape clustering",
                formula: "Density-based region growing",
                assumptions: "Cluster density",
                pros: "Noise tolerant, shape-flexible",
                cons: "Fails on varying density",
                whenNotToUse: "Sparse high-dim data",
                realWorldExample: "Geo-spatial clustering",
                complexity: "medium",
                detailedDescription: "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups together points that are closely packed together, marking points in low-density regions as outliers. Unlike K-means, it can find arbitrarily shaped clusters and doesn't require specifying the number of clusters.",
                additionalUseCases: [
                    "Identifying crime hotspots",
                    "Traffic pattern analysis",
                    "Anomaly detection",
                    "Image segmentation",
                    "Astronomical data analysis",
                    "Social network analysis",
                    "Retail store location optimization",
                    "Environmental monitoring"
                ],
                technicalDetails: "Two key parameters: epsilon (neighborhood radius) and minPts (minimum points to form cluster). Points classified as core, border, or noise. Works well with spatial indices (K-d tree, R-tree) for efficiency. Sensitive to parameter choice but robust to outliers.",
                preprocessing: "Feature scaling important. Choose eps and minPts carefully (use k-distance graph).",
                variants: "HDBSCAN (hierarchical DBSCAN), OPTICS, DENCLUE"
            },
            {
                name: "Isolation Forest",
                type: "Unsupervised",
                bestUse: "Anomaly detection",
                formula: "Random partitioning + path length",
                assumptions: "Anomalies are rare and different",
                pros: "Fast, handles high dimensions, no distance metrics",
                cons: "Sensitive to contamination parameter, struggles with local anomalies",
                whenNotToUse: "When anomalies cluster together",
                realWorldExample: "Credit card fraud detection",
                complexity: "low",
                detailedDescription: "Isolation Forest detects anomalies by isolating observations through random partitioning. It builds an ensemble of isolation trees where anomalies have shorter average path lengths than normal points because they are easier to isolate. The algorithm is efficient and particularly effective for high-dimensional datasets.",
                additionalUseCases: [
                    "Network intrusion detection",
                    "Manufacturing defect detection",
                    "Healthcare outlier identification",
                    "Banking transaction monitoring",
                    "System log anomaly detection",
                    "Sensor data quality monitoring",
                    "Insurance claim fraud",
                    "E-commerce fraud prevention"
                ],
                technicalDetails: "Builds multiple isolation trees using random feature selection and split values. Anomaly score based on average path length across all trees. Contamination parameter sets expected proportion of outliers. Performs well without distance calculations, making it suitable for high-dimensional data.",
                preprocessing: "Minimal preprocessing needed. No feature scaling required. Handle missing values. Set contamination parameter based on domain knowledge.",
                variants: "Extended Isolation Forest (EIF), SCiForest (uses split criteria), Robust Random Cut Forest"
            },
            {
                name: "Gaussian Mixture Models (GMM)",
                type: "Unsupervised",
                bestUse: "Probabilistic clustering",
                formula: "Sum of weighted Gaussian distributions",
                assumptions: "Data follows mixture of Gaussians",
                pros: "Soft clustering, provides probability estimates, flexible cluster shapes",
                cons: "Sensitive to initialization, assumes Gaussian distributions, needs cluster count",
                whenNotToUse: "Non-Gaussian distributed data",
                realWorldExample: "Customer segmentation with uncertainty",
                complexity: "medium",
                detailedDescription: "GMM is a probabilistic model that assumes data is generated from a mixture of multiple Gaussian distributions. Unlike K-Means which assigns each point to one cluster (hard clustering), GMM provides probability of membership for each cluster (soft clustering). It uses Expectation-Maximization (EM) algorithm to find optimal parameters.",
                additionalUseCases: [
                    "Image segmentation with uncertainty",
                    "Speaker identification in audio",
                    "Background subtraction in video",
                    "Density estimation",
                    "Feature extraction for classification",
                    "Anomaly detection with probability scores",
                    "Color quantization",
                    "Bioinformatics gene expression clustering",
                    "Financial market regime detection"
                ],
                technicalDetails: "Uses EM algorithm: E-step calculates probability of cluster membership, M-step updates parameters (means, covariances, weights). Covariance type can be full, tied, diagonal, or spherical. Model selection using BIC or AIC criteria. Can handle overlapping clusters naturally.",
                preprocessing: "Feature scaling recommended. Choose number of components (clusters) using information criteria. Handle missing values before training.",
                variants: "Variational Gaussian Mixture, Bayesian GMM, Dirichlet Process GMM"
            },
            {
                name: "t-SNE",
                type: "Dimensionality Reduction",
                bestUse: "High-dimensional data visualization",
                formula: "Minimize KL divergence between distributions",
                assumptions: "Local structure preservation important",
                pros: "Excellent visualizations, preserves local structure, reveals clusters",
                cons: "Computationally expensive, non-deterministic, doesn't preserve global structure",
                whenNotToUse: "Need consistent results, large datasets (>10k samples), interpretable features",
                realWorldExample: "Visualizing word embeddings",
                complexity: "high",
                detailedDescription: "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique primarily used for visualizing high-dimensional data in 2D or 3D space. It converts similarities between data points to joint probabilities and minimizes the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.",
                additionalUseCases: [
                    "Image dataset visualization",
                    "Single-cell RNA sequencing analysis",
                    "Customer behavior pattern visualization",
                    "Exploring deep learning features",
                    "Genomic data exploration",
                    "Social network community detection visualization",
                    "Drug discovery compound visualization",
                    "Handwritten digit visualization (MNIST)",
                    "Text document clustering visualization"
                ],
                technicalDetails: "Uses Student t-distribution in low-dimensional space to alleviate crowding problem. Perplexity parameter (5-50 typical) balances local vs global structure. Computationally expensive O(n¬≤) but approximations exist (Barnes-Hut). Results vary with different random initializations. Not suitable for dimensionality reduction followed by other ML algorithms.",
                preprocessing: "PCA preprocessing recommended for very high dimensions (>50). Standardize features. Remove duplicates. Perplexity should be lower than number of points.",
                variants: "Barnes-Hut t-SNE (faster), Parametric t-SNE, Multi-scale t-SNE, openTSNE"
            },
            {
                name: "AdaBoost",
                type: "Supervised",
                bestUse: "Ensemble classification",
                formula: "Weighted combination of weak learners",
                assumptions: "Weak learners better than random",
                pros: "Less prone to overfitting, simple to implement, works with any classifier",
                cons: "Sensitive to noisy data and outliers, sequential training (slow)",
                whenNotToUse: "Data has significant noise or outliers",
                realWorldExample: "Face detection (Viola-Jones)",
                complexity: "medium",
                detailedDescription: "AdaBoost (Adaptive Boosting) is an ensemble meta-algorithm that combines multiple weak classifiers to create a strong classifier. It works by sequentially training classifiers on weighted versions of the data, where misclassified samples receive higher weights. Each classifier's influence on the final prediction is based on its accuracy, allowing the ensemble to focus on difficult examples.",
                additionalUseCases: [
                    "Object detection in images",
                    "Text classification",
                    "Medical diagnosis",
                    "Pedestrian detection",
                    "Spam filtering",
                    "Customer churn prediction",
                    "Credit risk assessment",
                    "Biometric authentication",
                    "Quality control in manufacturing"
                ],
                technicalDetails: "Most commonly uses decision stumps (depth-1 trees) as weak learners. Adjusts weights of misclassified examples after each iteration. Final prediction is weighted vote of all classifiers. Weight of each classifier proportional to its accuracy. Typically needs 50-500 iterations. Works best when weak learners have accuracy >50%.",
                preprocessing: "Handle outliers carefully (they get high weights). Normalize features. Balance classes if needed. Works with categorical and numerical data.",
                variants: "AdaBoost.M1, AdaBoost.M2, SAMME (multi-class), SAMME.R (real-valued predictions), LogitBoost"
            },
            {
                name: "ElasticNet",
                type: "Supervised",
                bestUse: "Regularized regression with feature selection",
                formula: "L1 + L2 regularization penalty",
                assumptions: "Linear relationship, some features correlated",
                pros: "Feature selection + handles collinearity, stable with correlated features",
                cons: "Requires tuning two parameters, computationally more expensive than simple regression",
                whenNotToUse: "All features clearly important, no multicollinearity",
                realWorldExample: "Gene expression prediction",
                complexity: "low",
                detailedDescription: "ElasticNet is a regularized regression method that linearly combines L1 (Lasso) and L2 (Ridge) penalties. It overcomes limitations of both Lasso and Ridge: it can select groups of correlated features (unlike Lasso which arbitrarily picks one) while maintaining Ridge's stability. The mixing parameter alpha controls the L1/L2 ratio, providing flexibility in feature selection strength.",
                additionalUseCases: [
                    "High-dimensional genomic data analysis",
                    "Financial modeling with correlated predictors",
                    "Real estate price prediction with many features",
                    "Marketing response modeling",
                    "Medical outcome prediction",
                    "Climate modeling",
                    "Text mining with many features",
                    "Metabolomics data analysis",
                    "Portfolio optimization"
                ],
                technicalDetails: "Combines L1 penalty (sum of absolute coefficients) and L2 penalty (sum of squared coefficients). Two hyperparameters: alpha (mixing parameter, 0=Ridge, 1=Lasso) and lambda (overall regularization strength). Uses coordinate descent for optimization. Particularly useful when number of features exceeds number of samples or features are highly correlated.",
                preprocessing: "Standardize features (crucial for regularization). Handle missing values. Consider polynomial features if needed. Cross-validation for hyperparameter selection.",
                variants: "Sparse Elastic Net, Group Elastic Net, Adaptive Elastic Net"
            }
        ];

        let filteredAlgorithms = [...algorithms];

        function renderAlgorithms() {
            const grid = document.getElementById('algorithmGrid');
            const typeFilter = document.getElementById('typeFilter').value;
            const searchTerm = document.getElementById('searchBox').value.toLowerCase();
            const complexityFilter = document.getElementById('complexityFilter').value;

            filteredAlgorithms = algorithms.filter(algo => {
                const matchesType = typeFilter === 'all' || algo.type.includes(typeFilter);
                const matchesSearch = searchTerm === '' || 
                    algo.name.toLowerCase().includes(searchTerm) ||
                    algo.bestUse.toLowerCase().includes(searchTerm) ||
                    algo.realWorldExample.toLowerCase().includes(searchTerm);
                const matchesComplexity = complexityFilter === 'all' || algo.complexity === complexityFilter;
                
                return matchesType && matchesSearch && matchesComplexity;
            });

            if (filteredAlgorithms.length === 0) {
                grid.innerHTML = `
                    <div class="no-results">
                        <h2>üîç No algorithms found</h2>
                        <p>Try adjusting your filters or search terms</p>
                    </div>
                `;
            } else {
                grid.innerHTML = filteredAlgorithms.map((algo, index) => `
                    <div class="algorithm-card" tabindex="0" role="button" 
                         onclick="showDetail(${algorithms.indexOf(algo)})"
                         onkeypress="handleCardKeyPress(event, ${algorithms.indexOf(algo)})"
                         aria-label="View details for ${algo.name}">
                        <div class="card-header">${algo.name}</div>
                        <div class="card-type">${algo.type}</div>
                        <div class="card-content">
                            <div class="info-section">
                                <div class="info-label">Best Use Case</div>
                                <div class="info-value use-case">${algo.bestUse}</div>
                            </div>
                            <div class="info-section">
                                <div class="info-label">Formula / Logic</div>
                                <div class="info-value formula">${algo.formula}</div>
                            </div>
                            <div class="info-section">
                                <div class="info-label">‚úÖ Pros</div>
                                <div class="info-value pros">${algo.pros}</div>
                            </div>
                            <div class="info-section">
                                <div class="info-label">‚ö†Ô∏è Cons</div>
                                <div class="info-value cons">${algo.cons}</div>
                            </div>
                            <div class="info-section">
                                <div class="info-label">Real-World Example</div>
                                <div class="info-value">${algo.realWorldExample}</div>
                            </div>
                            <div class="info-section">
                                <div class="info-label">Complexity</div>
                                <span class="complexity-indicator complexity-${algo.complexity}">
                                    ${algo.complexity.charAt(0).toUpperCase() + algo.complexity.slice(1)}
                                </span>
                            </div>
                            <button class="expand-btn" onclick="event.stopPropagation(); showDetail(${algorithms.indexOf(algo)})" 
                                    onkeypress="handleButtonKeyPress(event, ${algorithms.indexOf(algo)})">
                                üìñ View Full Details
                            </button>
                        </div>
                    </div>
                `).join('');
            }

            updateStats();
        }

        function handleCardKeyPress(event, index) {
            if (event.key === 'Enter' || event.key === ' ') {
                event.preventDefault();
                showDetail(index);
            }
        }

        function handleButtonKeyPress(event, index) {
            if (event.key === 'Enter' || event.key === ' ') {
                event.preventDefault();
                event.stopPropagation();
                showDetail(index);
            }
        }

        function showDetail(index) {
            const algo = algorithms[index];
            const modal = document.getElementById('detailModal');
            const modalTitle = document.getElementById('modalTitle');
            const modalBody = document.getElementById('modalBody');

            modalTitle.textContent = algo.name;
            modalBody.innerHTML = `
                <div class="info-section">
                    <div class="info-label">Type</div>
                    <div class="info-value card-type" style="display: inline-block;">${algo.type}</div>
                    <span class="complexity-indicator complexity-${algo.complexity}" style="margin-left: 10px;">
                        ${algo.complexity.charAt(0).toUpperCase() + algo.complexity.slice(1)} Complexity
                    </span>
                </div>

                <div class="info-section">
                    <div class="info-label">Description</div>
                    <div class="info-value">${algo.detailedDescription}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">Key Formula / Logic</div>
                    <div class="info-value formula">${algo.formula}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">Best Use Case</div>
                    <div class="info-value use-case">${algo.bestUse}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">Assumptions</div>
                    <div class="info-value">${algo.assumptions}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">‚úÖ Advantages</div>
                    <div class="info-value pros">${algo.pros}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">‚ö†Ô∏è Disadvantages</div>
                    <div class="info-value cons">${algo.cons}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">‚ùå When NOT to Use</div>
                    <div class="info-value cons">${algo.whenNotToUse}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">Technical Details</div>
                    <div class="info-value">${algo.technicalDetails}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">Preprocessing Requirements</div>
                    <div class="info-value">${algo.preprocessing}</div>
                </div>

                <div class="info-section">
                    <div class="info-label">Variants & Related Algorithms</div>
                    <div class="info-value">${algo.variants}</div>
                </div>

                <div class="example-box">
                    <div class="example-title">üåü Primary Real-World Example</div>
                    <p>${algo.realWorldExample}</p>
                </div>

                <div class="info-section">
                    <div class="info-label">Additional Use Cases</div>
                    <ul class="additional-uses">
                        ${algo.additionalUseCases.map(use => `<li>${use}</li>`).join('')}
                    </ul>
                </div>
            `;

            modal.classList.add('show');
            document.body.style.overflow = 'hidden';
            
            // Focus trap and accessibility
            const closeBtn = modal.querySelector('.close-modal');
            closeBtn.focus();
        }

        function closeModal() {
            const modal = document.getElementById('detailModal');
            modal.classList.remove('show');
            document.body.style.overflow = 'auto';
        }

        function showExplainer() {
            const popup = document.getElementById('explainerPopup');
            const overlay = document.getElementById('overlay');
            popup.classList.add('show');
            overlay.classList.add('show');
            document.body.style.overflow = 'hidden';
            
            // Focus on close button for accessibility
            const closeBtn = popup.querySelector('.close-explainer');
            closeBtn.focus();
        }

        function closeExplainer() {
            const popup = document.getElementById('explainerPopup');
            const overlay = document.getElementById('overlay');
            popup.classList.remove('show');
            overlay.classList.remove('show');
            document.body.style.overflow = 'auto';
        }

        function handleInfoKeyPress(event) {
            if (event.key === 'Enter' || event.key === ' ') {
                event.preventDefault();
                showExplainer();
            }
        }

        function showComplexityExplainer() {
            const popup = document.getElementById('complexityPopup');
            const overlay = document.getElementById('overlay');
            popup.classList.add('show');
            overlay.classList.add('show');
            document.body.style.overflow = 'hidden';
            
            const closeBtn = popup.querySelector('.close-explainer');
            closeBtn.focus();
        }

        function closeComplexityExplainer() {
            const popup = document.getElementById('complexityPopup');
            const overlay = document.getElementById('overlay');
            popup.classList.remove('show');
            overlay.classList.remove('show');
            document.body.style.overflow = 'auto';
        }

        function handleComplexityKeyPress(event) {
            if (event.key === 'Enter' || event.key === ' ') {
                event.preventDefault();
                showComplexityExplainer();
            }
        }

        function showSelectionGuide() {
            const popup = document.getElementById('selectionPopup');
            const overlay = document.getElementById('overlay');
            popup.classList.add('show');
            overlay.classList.add('show');
            document.body.style.overflow = 'hidden';
            
            const closeBtn = popup.querySelector('.close-explainer');
            closeBtn.focus();
        }

        function closeSelectionGuide() {
            const popup = document.getElementById('selectionPopup');
            const overlay = document.getElementById('overlay');
            popup.classList.remove('show');
            overlay.classList.remove('show');
            document.body.style.overflow = 'auto';
        }

        function showMetricsGuide() {
            const popup = document.getElementById('metricsPopup');
            const overlay = document.getElementById('overlay');
            popup.classList.add('show');
            overlay.classList.add('show');
            document.body.style.overflow = 'hidden';
            
            const closeBtn = popup.querySelector('.close-explainer');
            closeBtn.focus();
        }

        function closeMetricsGuide() {
            const popup = document.getElementById('metricsPopup');
            const overlay = document.getElementById('overlay');
            popup.classList.remove('show');
            overlay.classList.remove('show');
            document.body.style.overflow = 'auto';
        }

        function handleHelperKeyPress(event, type) {
            if (event.key === 'Enter' || event.key === ' ') {
                event.preventDefault();
                if (type === 'selection') {
                    showSelectionGuide();
                } else if (type === 'metrics') {
                    showMetricsGuide();
                }
            }
        }

        function updateStats() {
            document.getElementById('totalCount').textContent = algorithms.length;
            document.getElementById('visibleCount').textContent = filteredAlgorithms.length;
        }

        // Event listeners
        document.getElementById('typeFilter').addEventListener('change', renderAlgorithms);
        document.getElementById('searchBox').addEventListener('input', renderAlgorithms);
        document.getElementById('complexityFilter').addEventListener('change', renderAlgorithms);

        // Close modal on background click
        document.getElementById('detailModal').addEventListener('click', function(e) {
            if (e.target === this) {
                closeModal();
            }
        });

        // Overlay click closes all popups
        document.getElementById('overlay').addEventListener('click', function() {
            closeExplainer();
            closeComplexityExplainer();
            closeSelectionGuide();
            closeMetricsGuide();
        });

        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') {
                closeModal();
                closeExplainer();
                closeComplexityExplainer();
                closeSelectionGuide();
                closeMetricsGuide();
            }
        });

        // Initialize
        renderAlgorithms();
    </script>
</body>
</html>